\documentclass{article}
%build with recipe latexmk
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{fancyhdr}
\pagestyle{fancy}
%\addtolength{\headwidth}{\marginparwidth}
%\addtolength{\headwidth}{\marginparsep}
%\addtolength{\headwidth}{\marginparsep}
\usepackage{tcolorbox}
\tcbuselibrary{theorems}
\usepackage{babel}
\usepackage{enumerate}
\usepackage{amsmath, amssymb, amsthm}
%\usepackage{a4wide}
\usepackage{float}
\usepackage{bbm}
\usepackage{tikz-cd}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{wrapfig}
\graphicspath{ {./images/} }
\usepackage{setspace}
\setstretch{1.1}
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=black,  %choose some color if you want links to stand out
}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{example}{Example}[section]
\newtheorem{defn}{Definition}[section]



\title{Part II - Stochastic Financial Models
    \\ \large
    Lectured by Dr M. R. Tehranchi
}

\author{Artur Avameri}
\date{Michaelmas 2022}

% figure support
\usepackage{import}
\usepackage{xifthen}
\pdfminorversion=7
\usepackage{pdfpages}
\usepackage{transparent}
\newcommand{\incfig}[1]{%
    \def\svgwidth{\columnwidth}
    \import{./figures/}{#1.pdf_tex}
}

\pdfsuppresswarningpagegroup=1

\setcounter{section}{-1}
\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Introduction}
\marginpar{07 Oct 2022, Lecture 1}


The goal of the course is to discuss some financial models. We will put ourself in the position of an investor and ask how we should invest. To do this, we first need to make some simplifying assumptions:
\begin{itemize}
    \item No dividends will ever be paid.
    \item We can buy a continuous spectrum of shares, not only integer amounts.
    \item There is no bid-ask spread, i.e. each stock only has one price value.
    \item Our actions have no price impact.
    \item There are no transaction costs.
    \item There are no short-selling constraints.
\end{itemize}

For the next few lectures, we will only consider $t \in \{0,1\}$, i.e. one-period models. After that, we will consider $t \in \{0,1,\ldots\}$, i.e. discrete-time models, and at the end of the course, $t \in \mathbb{R}_{\ge 0}$, i.e. continuous-time models.

\section{One-period models}

Setup: There are $d$ risky assets, and the value of the $i^{\text{th}}$ asset at time $t$ is denoted by $S^i_t$, hence we have a vector of prices $S_t = (S^1_t, \ldots, S^d_t)^{\text{T}} \in \mathbb{R}^d$. There is also a risk-free asset, which pays a constant interest rate $r$. 

\vspace{1mm}

Introduce an investor who picks a portfolio at time 0. We say $\theta^i$ is the number of shares bought at time 0 (if $\theta^i<0$, the investor shorts $|\theta^i|$ shares) and $\theta^0$ is the number of shares the investor has of the risk-free asset.

\vspace{1mm}
    
Let $X_t$ be the investor's wealth at time $t$. We hence have 
\begin{align*}
    X_0 = \theta^0 + \sum_{i=1}^{d} \theta^i S^i_0 = \theta^0 + \theta^{\text{T}}S_0 \\ X_1 = \theta^0(1+r) + \theta^{\text{T}}S_1.
\end{align*}
Eliminating $\theta^0$, we get \[
X_1 = (1+r)X_0  + \theta^{\text{T}}(S_1 - (1+r)S_0).
\]

\textbf{Risk.} $r$ and $S_0$ are known constants. $S_1$ is unknown. We will also assume that we are certain of the distribution of $S_1$.

\subsection{Mean-variance analysis}

\textbf{Mean-variance portfolio problem.} (Markovitz, 1952). Given $x$ and $m$, find $\theta \in \mathbb{R}^d$ to minimize $\text{Var}(X_1)$ subject to $X_0 = x$ and $\mathbb{E}[X_1]\ge m$.

Draw a graph of variance and mean on the $x-y$ plane. We get a subset on the right-hand side of the plane consisting of points of the form $(\text{Var}(X_1), \mathbb{E}[X_1])$.

\begin{defn}[Mean-variance efficient frontier]
    The mean-variance efficient frontier is just the left boundary of the above set, i.e. for every value of $m$, the strategy that minimizes variance while achieving $\mathbb{E}[X_1]=m$.
\end{defn}
\begin{defn}
    A portfolio is \textbf{mean-variance efficient} if it is the optimal solution to a mean-variance portfolio problem for some $x,m$.
\end{defn}

\textbf{Notation/assumptions.} Assume that $S_1$ is square integrable, let:
\begin{itemize}
    \item $\mathbb{E}[S_1]= \mu \in \mathbb{R}^d$
    \item $\text{Cov}(S_1) = V$, a $d \times d$ matrix, which we assume is positive definite (it is always nonnegative definite, but we want it to be invertible). We have $V  =\mathbb{E}[(S_1- \mu)(S_1 - \mu)^{\text{T}}] = \mathbb{E}[S_1S_1^{\text{T}}]-\mu \mu^{\text{T}}$.
    \item Assume also that $\mu \neq (1+r)S_0$.
\end{itemize}

\begin{theorem}
    The optimal solution to the mean-variance portfolio problem is
    \begin{align*}
        \theta^* = \lambda V^{-1}(\mu-(1+r)S_0) \text{,   where } \\
        \lambda = \frac{(m-(1+r)x)^{+}}{(\mu-(1+r)S_0)^\top V^{-1} (\mu-(1+r)S_0)}
    \end{align*}
\end{theorem}
\textbf{Notation.} For $a \in \mathbb{R}$, we have $a^+ = \max(0,a)$.

\marginpar{10 Oct 2022, Lecture 2}

Before we prove this, let's dwell on it for a bit.

\begin{cor}[Mutual fund theorem]
    A portfolio is mean-variance efficient if and only if it is a nonnegative scalar multiple of $\theta_{\text{mar}} = V^{-1}(\mu - (1+r)S_0)$.
\end{cor}
\begin{defn}
    $\theta_{\text{mar}} = V^{-1}(\mu - (1+r)S_0)$ is called the \textbf{market portfolio}.
\end{defn}

\textbf{Remark.} What is the origin of the name "market portfolio"? Suppose there are $n_i$ shares of asset $i$, $1 \le i \le d$. Suppose there are $K$ investors and investor $k$ holds portfolio $\theta_k$. Market clearing (i.e. total supply equals total demand) implies $\sum_{k=1}^{K} \theta_k = n = \begin{pmatrix} n_1 & \ldots & n_d \end{pmatrix}^{\text{T}}$.

Make an additional assumption: assume all investors agree on $\mu$ and $V$ and hold a mean-variance efficient portfolio. Then by the mutual fund theorem, $\theta_k = \lambda_k \theta_{\text{mar}}$ for some scalar $\lambda_k \ge 0$, hence $n = \Lambda \theta_{\text{mar}}$, where $\Lambda = \sum_{}^{} \lambda_k$. So $\theta_{\text{mar}}$, the market portfolio, is really just some scaling of the entire market by some $\Lambda$.

\newpage
\textbf{Notation for the proof.}
\begin{itemize}
    \item $\mathbb{E}[X_1] = (1+r)x + \theta^{\text{T}}(\mu-(1+r)S_0)$
    \item $\text{Var}(X_1) = \theta^{\text{T}} V \theta$
\end{itemize}
So we want to minimize $\frac{1}{2} \theta^{\text{T}} V \theta$ subject to $\theta^{\text{T}}(\mu - (1+r)S_0) \ge m - (1+r)x$.

Note that if we wrote $a = \mu - (1+r)S_0$ and $b = m-(1+r)x$, the condition above would just be $a^{\text{T}}\theta \ge b$. Note also that
\begin{itemize}
    \item $(\theta^*)^{\text{T}}(\mu-(1+r)S_0) = (m - (1+r)x)^+ \ge  m -(1+r)x.$ \newline In IB Optimization terms, this is primal feasibility. Using the above notation, this is also $a^{\text{T}}\theta^* \ge b$.
    \item $\lambda \ge 0$ (dual feasibility).
    \item $\lambda(m-(1+r)x - (\theta^*)^{\text{T}}(\mu - (1+r)S_0)) \color{red} = 0 \color{black}$. This is $\lambda(b-a^{\text{T}}\theta^*) = 0$, i.e. just complementary slackness.  
\end{itemize}
\begin{proof}
    Now pick a feasible portfolio $\theta$. Note $\frac{1}{2}\theta^{\text{T}} V \theta \ge \frac{1}{2} \theta^{\text{T}} V \theta + \lambda(b - a^{\text{T}}\theta)$ since $\theta$ is feasible. Write this as 
    \begin{align*}
        &\frac{1}{2}\theta^{\text{T}} V \theta \ge \frac{1}{2} \theta^{\text{T}} V \theta + \lambda(b - a^{\text{T}}\theta) =\\ 
        & = \frac{1}{2}(\theta-\theta^*)^{\text{T}} V (\theta-\theta^*) + \lambda b - \frac{1}{2}(\theta^*)^{\text{T}} V \theta^* \ge  \lambda b - \frac{1}{2}(\theta^*)^{\text{T}} V \theta^*   
    \end{align*}
    
    where $\theta^* = \lambda V^{-1} a$, the last inequality follows from $V$ being positive definite, and with equality if and only if $\theta = \theta^*$. (This is just the proof of the Lagrangian sufficiency theorem).
\end{proof}

\subsection{Capital Asset pricing model}
Recall a fact from probability: let $X,Y$ be square-integrable random variables (i.e. they have finite variances) with $\text{Var}(X)>0$. Then there exist unique numbers $a,b$ such that $Y = a + bX + Z$ where $\mathbb{E}[Z]=0$ and $\text{Cov}(X,Z) = 0$.
\begin{proof}
    Let $Z = Y - a - bX$ for $a,b$ to be determined. Then 
    \begin{align*}
        \mathbb{E}[Z] = 0 &\implies \mathbb{E}[Y] =a + b \mathbb{E}[X] \\
        \text{Cov}(X,Z) = 0 &\implies \text{Cov}(Y,X) = b \text{Var}(X),
    \end{align*}
    which has a unique solution $b = \frac{\text{Cov}(Y,X)}{\text{Var}(X)}$ and $a = \mathbb{E}[Y] - b \mathbb{E}[X]$.
\end{proof}
\textbf{Remark.} Alternatively, the given values of $a$ and $b$ are the unique minimizers of $\mathbb{E}[(Y-a-bX)^2]$.

Setup: Our investor has initial wealth $X_0$ and portfolio $\theta$. We still have ${X_1 = (1+r)X_0 + \theta^{\text{T}}(S_1 - (1+r)S_0)}$. Market capitalization will be proportional to $\theta_{\text{mar}}^{\text{T}}S_0$ at time 0 and $\theta_{\text{mar}}^{\text{T}}S_1$ at time 1.

\begin{theorem}
    $$X_1 - (1+r)X_0 = b(X_1^{\text{mar}} - (1+r)X_0^{\text{mar}}) + Z$$ where $\mathbb{E}[Z]=0$ and $\text{Cov}(X_{\text{mar}},Z)=0$.
\end{theorem}
\begin{theorem}[Reformulation with returns instead of wealths]
    Let $R = \frac{X_1}{X_0} - 1$, $R_{\text{mar}} = \frac{X_1^{\text{mar}}}{X_0^{\text{mar}}} - 1$. Then alpha equals zero in a mean-variance efficient market. In other words, if $R - r = \alpha + \beta(R_{\text{mar}} - r) + \epsilon$ where $\mathbb{E}[\epsilon] = 0, \text{Cov}(R_{\text{mar}},\epsilon)=0$, then $\alpha = 0$.
\end{theorem}
\textbf{Remark.} This says that if every market participant were mean variance efficient, then we can't "beat" the market index (everyone has the same portfolio up to scaling). This is the CAPM assumption.

\begin{proof}
    We have
    \[
        \mathbb{E}[X_1 - (1+r)X_0] = \theta^{\text{T}}(\mu-(1+r)S_0)    
    \]
    and
    \begin{multline*}
        \text{Cov}(X_1-(1+r)X_0, X_1^{\text{mar}}-(1+r)X_0^{\text{mar}}) = \theta^{\text{T}} \text{Cov}(S_1)\theta_{\text{mar}} =\\ \theta^{\text{T}} V V^{-1} (\mu-(1+r)S_0) = \mathbb{E}[X_1 - (1+r)X_0].
    \end{multline*}
    Choosing $\theta=\theta_{\text{market}}$ in the calculation, we get $$\mathbb{E}[X_1^\text{mar} - (1+r)X_0^{\text{mar}}] = \text{Var}(X_1^{\text{mar}} - (1+r)X_0^{\text{mar}}).$$ 
    Therefore $$a = \mathbb{E}[X_1 - (1+r)X_0] - b \mathbb{E}[X_1^{\text{mar}} - (1+r)X_0^{\text{mar}}],$$ 
    but $$b = \frac{\text{Cov}(X_1-(1+r)X_0,X_1^{\text{mar}}-(1+r)X_0^{\text{mar}})}{\text{Var}(X_1^{\text{mar}}-(1+r)X_0^{\text{mar}})} = \frac{\mathbb{E}[X_1 - (1+r)X_0]}{\mathbb{E}[X_1^{\text{mar}} - (1+r)X_0^{\text{mar}}]},$$
    so $a = 0$.
\end{proof}

\textbf{Remark.} Markowitz introduced mean-variance analysis, and Sharpe introduced CAPM. They won the Nobel prize in 1990.

\newpage

\marginpar{12 Oct 2022, Lecture 3}

This tells us that \[
\mathbb{E}[X_1] = (1+r)X_0 + \left(\mathbb{E}[X_1^{\text{mar}}]-(1+r)X_0^{\text{mar}} \right) \frac{\text{Cov}(X_1^{\text{mar}},X_1)}{\text{Var}(X_1^{\text{mar}})},
\]
i.e. $\alpha = 0$, i.e. $\theta^{\text{mar}}= \text{const}\cdot (n_1 \ldots n_d)^{\text{T}}$.
\vspace{1mm}

By similar reasoning to previous analysis, the optimal solution to "minimize $\text{Var}(X_1)$ wrt $X_0 =x$ and $\mathbb{E}[X_1] = m$ (notice the equals sign here instead of the inequality) is $\theta^* = \lambda \theta^{\text{mar}}$, where as before, $\theta^\text{mar} = V^{-1}(\mu - (1+r)S_0)$, but this time \[
\lambda = \frac{m-(1+r)x}{(\mu-(1+r)S_0)^{\text{T}}V^{-1}(\mu-(1+r)S_0)}
\]
(note the lack of positive part in the numerator).
Plugging this in for variance, we get that the minimized variance is 
\[
\text{min}\text{Var}(X_1) = \frac{(m-(1+r)x)^2}{(\mu-(1+r)S_0)^{\text{T}}V^{-1}(\mu-(1+r)S_0)},
\]
i.e. the efficient frontier is a parabola.

\subsection{Expected utility}

So far, we have implicitly assumed that an agent prefers $X$ to $Y$ if either 
\begin{itemize}
    \item $\mathbb{E}[X] > \mathbb{E}[Y]$ and $\text{Var}(X)=\text{Var}(Y)$, or
    \item $\mathbb{E}[X]=\mathbb{E}[Y]$ and $\text{Var}(X)<\text{Var}(Y)$.
\end{itemize}

\textbf{Aside:} This is rather crude. For example, the St Petersburg paradox - toss a coin until you get heads, if it took $n$ tries, I pay you $2^n$ pounds. How much should you pay to play? The expected value is infinite, yet you obviously wouldn't pay me very much. End of aside.
\vspace{1mm}

The solution to this is to instead consider the \textbf{expected utility} of the payout.\footnote{In the real world, another solution is to consider counterparty risk: surely most people will not be actually be in a situation to give you $2^{40}$ pounds, so you should never pay more than 40 pounds to play.}

\begin{defn}
    The \textbf{expected utility hypothesis} says that every agent has a \textbf{utility function} $U$ such that the agent prefers random payout $X$ to $Y$ if and only if $\mathbb{E}[U(X)] > \mathbb{E}[U(Y)]$. If $\mathbb{E}[U(X)] = \mathbb{E}[U(Y)]$, we say the agent is \textbf{indifferent} between $X$ and $Y$.
\end{defn}

\textbf{Aside:} We can show (under some assumptions) that every agent must have a utility function. For random payouts $X,Y,Z$ and $A$ an event independent from $X,Y,Z$, we assume the von Neumann -- Morgenstern axioms:
\begin{itemize}
    \item Either $X \succ Y, X \prec Y$ or $X \sim Y$ (Completeness).
    \item $X \succ Y$ and $Y \succ Z \implies X \succ Z$ (Transitivity).
    \item $X \succeq Y$ if and only if $\mathbbm{1}_A X  + \mathbbm{1}_{A^C} Z \succeq \mathbbm{1}_A Y + \mathbbm{1}_{A^C} Z$ (Independence).
    \item If $X \succeq Y \succeq Z$, then $\exists p \in [0,1]$ s.t. if $\mathbb{P}(A)=p$, then $Y = X \mathbbm{1}_A + Z \mathbbm{1}_{A^C}$ (Continuity).
\end{itemize}
vN-M proved in 1947 that an agent has expected utility preferences if and only if the vN-M axioms (plus another technical axiom) hold. End of aside.

\subsection{Risk-aversion}
We have two further usual assumptions about our utility function $U$.
\begin{itemize}
    \item Increasing: $x>y \implies U(x)>U(y)$. Even for random variables, if $\mathbb{P}(X>Y)=1$, then $\mathbb{E}[U(X)] > \mathbb{E}[U(Y)]$.
    \item Concavity: $U(px+qy) \ge p U(x) + q U(y)$ for any $x,y, 0\le p = 1-q \le 1$.
    \vspace{1mm}

    Recall Jensen's inequality: $\mathbb{E}[U(X)] \le U(\mathbb{E}[X])$ for a concave function $U$. Hence if $U$ is concave, then $\mathbb{E}[X] \succeq X$ for any payout $X$. 
    \vspace{1mm}
    
    Also note that $U$ concave implies $U'$ is decreasing (where we can think of $U'$ as the \textbf{marginal utility}). Furthermore, $U' > 0$ (as the function is increasing) and $U'' < 0$.
\end{itemize}
\begin{defn}
    The Arrow-Pratt \textbf{coefficient of absolute risk aversion } is $$\frac{-U''(x)}{U'(x)}.$$
\end{defn}
\begin{defn}
    The Arrow-Pratt \textbf{coefficient of relative risk aversion} is \[
    \frac{xU''(x)}{U'(x)}.
    \] 
\end{defn}
The most important examples of utility functions are:
\begin{itemize}
    \item Exponential (CARA): $U(x) = -e^{-\gamma x}$ for $\gamma = \frac{-U''(x)}{U'(x)}.$
    \item Power (CRRA): $U(x) = \frac{1}{1-R}x^{1-R}$, $x>0, R\ge 0, R\neq1$ where $R=\frac{-xU''(x)}{U'(x)}$.
    \item Logarithmic: $U(x)= \log x$. This is CRRA with $R=1$.
\end{itemize}

\marginpar{14 Oct 2022, Lecture 4}
We have the \textbf{utility maximization problem}: Given a concave, increasing utility function $U$ and initial wealth $X_0=x$, find $\theta$ that maximizes $$\mathbb{E}[U(X_1)] = \mathbb{E}[U(x(1+r)-\theta^{\text{T}}(S_1-(1+r)S_0))].$$

\begin{theorem}
    Suppose $U$ is concave, increasing, and differentiable (i.e. "suitably nice"), then $$\mathbb{E}[U'(X_1^*)(S_1-(1+r)S_0)] = 0,$$ where $X_1^* = x(1+r) + (\theta^*)^{\text{T}}(S_1-(1+r)S_0)$ is the optimal time 1 wealth.
\end{theorem}
\textbf{Remark.} There's a hidden assumption here as well that everything is integrable, else the expected value could be infinite. 
\begin{proof}
    Differentiate the objective function wrt $\theta$ and pass the derivative inside the expectation.
\end{proof}

\textbf{Remark.} Notice that $S_0 = \frac{\mathbb{E}[U'(X_1^*)S_1]}{(1+r)\mathbb{E}[U'(X_1^*)]}$. So if $$Z = \frac{U'(X_1^*)}{(1+r)\mathbb{E}[U'(X_1^*)]},$$ then $\mathbb{E}[ZS_1] = S_0$ and $\mathbb{E}[Z]=\frac{1}{1+r}$.

\begin{defn}
    Given a market model, a \textbf{state price density} or \textbf{pricing kernel} is a positive random variable $Z$ such that $\mathbb{E}[ZS_1]=S_0$ and $\mathbb{E}[Z]=\frac{1}{1+r}$.
\end{defn}

Our above theorem now says that the marginal utility of maximized wealth is proportional to a state-price density for the model, i.e. our $Z$ above is a state-price model. 

We can recover today's prices in terms of the expectation of tomorrow's prices times the kernel (i.e. averaging tomorrow's prices with respect to this kernel).

Aside: the origin of the name. Consider a market model with $d$ outcomes (states) $\Omega = \{\omega_1,\ldots,\omega_d\}$. Suppose asset $i$ pays 1 if state $i$ happens and $0$ otherwise, so $S^i_1(\omega) = \begin{cases}
    1 &\text{ if } \omega=\omega_i\\
    0 &\text{ otherwise}
\end{cases}.$ Let $Z$ be a state-price density, then \[
\mathbb{E}[ZS_1] = \sum_{\omega \in \Omega}^{} Z(\omega)S_1^i(\omega)\mathbb{P}(\{\omega\}) = Z(\omega_i)\mathbb{P}(\{\omega_i\}) = S^i_0,
\]
so $Z(\omega_i) = \frac{S_0^i}{\mathbb{P}(\{\omega_i\})}$. End of aside.

\subsection{Risk-neutral measures}
\textbf{Setup:} Given a probability space ($\Omega, \mathcal{F}, \mathbb{P}$), with a set of outcomes $\Omega$, a set of events $\mathcal{F}$ (i.e. subsets of $\Omega$), and a probability measure $\mathbb{P} : \mathcal{F} \to [0,1]$.
\vspace{1mm}

Let $Y$ be a positive random variable such that $\mathbb{E}^{\mathbb{P}}[Y]=1$. Define a new probability measure $\mathbb{Q}$ by the formula \[
\mathbb{Q}(A) = \mathbb{E}^{\mathbb{P}}[Y \mathbbm{1}_A]
\] for any event $A \in \mathcal{F}$. It is a useful fact from measure theory that \[
\mathbb{E}^{\mathbb{Q}}[X] = \mathbb{E}^{\mathbb{P}}[YX]
\] for any $\mathbb{Q}$-integrable random variable $X$.
\vspace{1mm}

\textbf{Notation.} We write $Y = \frac{d \mathbb{Q}}{d \mathbb{P}}$, called the \textbf{density} of $\mathbb{Q}$ with respect to $\mathbb{P}$, or the likelihood ratio, or the Radon-Nikodym derivative.

\begin{defn}
    Two probability measures $\mathbb{P}$ and $\mathbb{Q}$ are \textbf{equivalent} if and only if $\mathbb{Q}(A) = 0 \iff \mathbb{P}(A) = 0$. Alternatively, $\mathbb{Q}(A) = 1 \iff \mathbb{P}(A) = 1$. Or $\mathbb{Q}(A) > 0 \iff \mathbb{P}(A) > 0$. Or even $\mathbb{Q}(A) < 1 \iff \mathbb{P}(A) < 1$.  
\end{defn}

A theorem we will not need, but explains the names above:
\begin{theorem}[Radon-Nikodym theorem]
    Probability measures $\mathbb{P}$ and $\mathbb{Q}$ that are defined on the same measurable space $(\Omega, \mathcal{F})$ are equivalent if and only if there exists a positive random variable $Y$ such that $\mathbb{Q}(A) = \mathbb{E}^{\mathbb{P}}[Y \mathbbm{1}_A]$ for all events $A$.
\end{theorem}

\begin{example}
    Suppose $\Omega = \{\omega_1, \omega_2, \ldots\}$. Suppose $\mathbb{P}(\omega_i)>0, \mathbb{Q}(\omega_i)>0 ~\forall i$. Then $\mathbb{P}$ and $\mathbb{Q}$ are equivalent, and $\frac{d\mathbb{Q}}{d\mathbb{P}}(\omega) = \frac{\mathbb{Q}(\omega)}{\mathbb{P}(\omega)}$.
\end{example}
\begin{example}
    Suppose $X \sim \text{Exp}(\lambda)$ defined on ($\Omega, \mathcal{F}, \mathbb{P}$). Set $Y = \frac{\mu}{\lambda}e^{(\lambda-\mu)X}$. Note $Y>0$ and 
    \[
    \mathbb{E}[f(X)] = \mathbb{E}^{\mathbb{P}}[Yf(X)] = \int_{0}^{\infty} \frac{\mu}{\lambda} e^{(\lambda-\mu)x} f(x) \lambda e^{-\lambda x} dx = \int_{0}^{\infty} \mu e^{-\mu x} f(x) dx,
    \] so $X$ has a new distribution $\text{Exp}(\mu)$ under $\mathbb{Q}$.
\end{example}

Let us now return to finance. Setup: prices $\{S_t\}_{t \in \{0,1\}}$ are defined on a probability space ($\Omega, \mathcal{F}, \mathbb{P}$). 

\begin{defn}
    A \textbf{risk-neutral measure} is any probability measure $\mathbb{Q}$ equivalent to $\mathbb{P}$ such that \[
    \mathbb{E}^{\mathbb{Q}}[S_1] = (1+r)S_0.
    \]
\end{defn}
\textbf{Remark.} Let $\mathbb{Q}$ be equivalent to $\mathbb{P}$ and set $Z =\frac{1}{1+r} \frac{d \mathbb{P}}{d \mathbb{Q}}$. Then $Z$ is a state-price density if and only if $\mathbb{Q}$ is risk-neutral.

So finally, the main theorem about utility maximization reads: if $\theta^*$ is optimal, then $U'(X_1^*)$ is proportional to the density of a risk-neutral measure.

\marginpar{17 Oct 2022, Lecture 5}
\vspace{1mm}

\textbf{Example} : Utility maximization in one-period a binomial model. We search for $\max \mathbb{E}[U(X_1)]$ given $X_1=x$ and $U$ a concave, increasing, continuously differentiable function. The market model has interest rate $r$, $d=1$ (there is only one risky asset), $S_0$ is given, and $$\mathbb{P}(S_1=(1+b)S_0) = p = 1- \mathbb{P}(S_1 = (1+a)S_0),$$
where $-1<a<b$ are given, and $p \in (0,1)$.

What are the risk-neutral measures? We need to find $\mathbb{Q} \sim \mathbb{P}$ such that $\mathbb{E}^{\mathbb{Q}}[S_1]=(1+r)S_0$. Let $q = \mathbb{Q}(S_1=(1+b)S_0)$. We need $q \in (0,1)$ and $q(1+b)S_0 + (1-q)(1+a)S_0 = (1+r)S_0$, so $q = \frac{r-a}{b-a}, 1-q = \frac{b-r}{b-a}$.
\vspace{1mm}

\textbf{Note.} A risk-neutral measure exists only if $a < r < b$ (i.e. this is necessary for a solution to exist to the utility maximization problem). Also, if we have a solution, it is unique.
\vspace{1mm}

From last time, we know that $U'(X_1^*) = \lambda \frac{d \mathbb{Q}}{d\mathbb{P}}$ for some $\lambda>0$, so $X_1^* = (1+r)x + \theta^*(S_1-(1+r)S_0)$, where $\theta^*$ is the optimal portfolio. 

So we know that
\[
\begin{cases}
    U'((1+r)x + \theta^*(b-r)S_0) = \lambda \frac{q}{p} \\
    U'((1+r)x + \theta^*(r-a)S_0) = \lambda \frac{1-q}{1-p}.
\end{cases}
\]
Let $I = (U')^{-1}$, i.e. the inverse of the marginal utility, where $I$ is decreasing and continuous. Hence we can look at our formula above and write it as $X_1^* = I(\lambda \frac{d \mathbb{P}}{d\mathbb{Q}})$. Now taking expected values, we find \[
\mathbb{E}^{\mathbb{Q}}[X_1^*] = (1+r)x + \theta^*\mathbb{E}[S_1-(1+r)S_0] = (1+r)x = \mathbb{E}^{\mathbb{Q}}\left[I\left(\lambda \frac{d \mathbb{P}}{d\mathbb{Q}}\right)\right].
\]
So the recipe to find the optimal solution is to let $\lambda$ solve $$\mathbb{E}^{\mathbb{Q}}\left[I \left(\lambda \frac{d \mathbb{P}}{d\mathbb{Q}}\right)\right] = (1+r)x,$$
i.e. $$q I\left(\lambda\frac{q}{p}\right) + (1-q)I\left(\lambda \frac{1-q}{1-p}\right),$$ and then solve either $U'((1+r)x + \theta^*(b-r)S_0) = \lambda \frac{q}{p}$ or $U'((1+r)x + \theta^*(a-r)S_0) = \lambda \frac{1-q}{1-p}$ to find $\theta^*$.
\vspace{1mm}

\textbf{Special case}: CRRA, $U(x) = \frac{1}{1-R}x^{1-R}$, $x>0, R>0, R \neq 1$. Then $U'(x)=x^{-R}$, so $I(y)=y^{\frac{-1}{R}}$, so in this case $\lambda$ solves $$\lambda^{\frac{-1}{R}} \mathbb{E}^{\mathbb{Q}}\left[\left(\frac{d \mathbb{P}}{d \mathbb{Q}}\right)^{\frac{-1}{R}}\right] = (1+r)x,$$
so \[
\lambda = ((1+r)x)^{-R}\left( \mathbb{E}^{\mathbb{Q}}\left[\frac{d \mathbb{P}}{d \mathbb{Q}}\right]^{\frac{-1}{R}}\right)^R,
\] and then plug it in to find $\theta^*$.

\subsection{Contingent claims}

\textbf{Contingent claims} are just another name for an asset (they are a derivative whose value depends on some other underlying asset). Idea: some assets are "fundamental", and some are contingent on the value of the fundamental asset.

\begin{example}
    A \textbf{call option} in the one period-case. Given $d=1$, a call option is the right, but not the obligation, to buy the stock at time $1$ for a fixed price (strike price) $K$.
\end{example}
The call option is a contingent claim and has a price $C_0$ at time 0. A question we'd like to answer (and later will) is: what is $C_0$? Note that if $S_1 > K$, it is rational to exercise the option, while if $S_1\le K$, it is rational not to exercise the option, i.e. the buyer's profit is $(S_1-k)^+$.

\subsubsection{Indifference price of contingent claims.} 

Consider a utility maximizing agent. We would prefer to buy one share of the claim with time 1 payout $Y$ at price $\pi$, if $\exists \theta$ such that $$\mathbb{E}[U((1+r)(x-\pi) + \theta^{\text{T}}(S_1 -(1+r)S_0)+ Y)] > \mathbb{E}[U((1+r)x + \phi^{\text{T}}(S_1-(1+r)S_0))] ~\forall \phi.$$

\textbf{Notation.} Let $\mathcal{X} = \{\theta^{\text{T}}(S_1-(1+r)S_0) ~|~ \theta \in \mathbb{R}^d\}$ be the time 1 wealths attainable in the market from 0 initial wealth.
\vspace{1mm}

\textbf{Assumption.} The model parameters are such that there exists an optimal solution to any utility maximization problem we consider.

\begin{defn}
    The \textbf{indifference price} $\pi(Y)$ of a contingent claim with time 1 payout $Y$ is the unique solution to 
    \[
    \max_{X \in \mathcal{X}} \mathbb{E}[U((1+r)(x-\pi)+X+Y)] = \max_{X \in \mathcal{X}} \mathbb{E}[U((1+r)x + X)],
    \] where $x = x_0$ is the time 0 initial wealth.     
\end{defn}

\marginpar{19 Oct 2022, Lecture 6}

We change our notation a little from last time and let 
\[
\mathcal{X}(x)=\{(1+r)x + \theta^\top(S_1-(1+r)S_0) ~|~ \theta \in \mathbb{R}^d\}
\]
be the set of time-1 wealths attainable from starting wealth $x$.
This lets us rewrite the above indifference price equation as \[
\max_{X \in \mathcal{X}(x)} \mathbb{E}[U(X+Y-(1+r)\pi)] = \max_{X \in \mathcal{X}(x)}\mathbb{E}[U(X)]
\]
\textbf{Remark.} $\pi \mapsto \max \mathbb{E}[U(X+Y-(1+r)\pi)]$ is decreasing. To see this, look at 
\begin{align*}
    \max \mathbb{E}[U(X+Y-(1+r)(\pi+\epsilon))] = \mathbb{E}[U(X_\epsilon + Y -(1+r)(\pi+\epsilon))] < \\
    \mathbb{E}[U(X_{\epsilon}+Y-(1+r)\pi)] \le \max_{X \in \mathcal{X}(x)} \mathbb{E}[U(X+Y-(1+r)\pi)].
\end{align*}
since $U$ is increasing (for $\epsilon>0$, $X_\epsilon$ the value achieving the maximum). Hence the solution is unique.

\textbf{Remark.} $\pi \mapsto \max \mathbb{E}[U(X+Y-(1+r)\pi)]$ is also continuous, but the proof it omitted.
\vspace{1mm}


Warmup: suppose $\mathbb{P}(Y \ge a) = 1$. Then $\pi(Y)\ge \frac{a}{1+r}$. 
\begin{proof}
    For $\pi(Y)$, the indifference price of $Y$, we have
    \begin{align*}
        \max \mathbb{E}[U(X)] =\max \mathbb{E}[U(X+Y-(1+r)\pi)] \ge \max \mathbb{E}[U(X+a-(1+r)\pi)] = \\ \mathbb{E}[U(X-(1+r)(\pi-\frac{a}{1+r}))].
    \end{align*}
    Hence $(\pi-\frac{a}{1+r})\ge 0$ from the remark above (the function is decreasing).
\end{proof}
\begin{theorem}
    The indifference pricing fuction $\pi$ is concave.
\end{theorem}
\begin{proof}
    Fix $Y_0, Y_1$, and let $Y_p = pY_1 + (1-p)Y_0$. Let $\pi_i = \pi(Y_i)$ for $i \in \{0,p,1\}, 0\le p\le 1$. Then $\max \mathbb{E}[U(X+Y_p - (1+r)\pi_p)] = \max \mathbb{E}[U(X)]$ by the definition of the indifference price, but 
    \begin{align*}
        &\max \mathbb{E}[U(X)] = \\ 
        (1-p) \max \mathbb{E}[U(X+Y_0 - (1+r)\pi_0)] &+ p \max \mathbb{E}[U(X+Y_1 -(1+r)\pi_1)] = \\
        (1-p)\mathbb{E}[U(X_0+Y_0-(1+r)\pi_0)] &+ p \mathbb{E}[U(X_1 + Y_1 - (1+r)\pi_1)] \le \\
        \mathbb{E}[U((1-p)X_0+pX_1 &+ Y_p -(1+r)(p \pi_1 + (1-p)\pi_0)]
    \end{align*}
    by concavity (where $X_0,X_1$ are maximizers of their respective expressions). Note $(1-p)X_0 + pX_1 \in \mathcal{X}(x)$, so the above is 
    \begin{align*}
        \le \max \mathbb{E}[U(X+Y_p -(1+r)(p \pi_1 +(1-p)\pi_0))].
    \end{align*}
    Hence $p \pi_1 + (1-p)\pi_0 \le \pi_p$ and we're done.
\end{proof}
\textbf{Remark.} $\pi(Y)$ is the buyer's indifference price (bid price). Note that the seller's indifference price is $-\pi(-Y)$ (ask price).
\vspace{1mm}

We predict that $\pi(Y) \le -\pi(-Y)$ (i.e. the bid price is less than the ask price). And this is true: \[
\frac{1}{2}\pi(Y) + \frac{1}{2}\pi(-Y) \le \pi\left(\frac{Y}{2}-\frac{Y}{2}\right) = \pi(0) = 0
\] by concavity.

\subsubsection{Marginal utility pricing}
Fix a claim with payout $Y$. Let $\pi_t = \frac{\pi(tY)}{t}$. On the example sheet, we will show that $t \mapsto \pi_t$ is decreasing (using concavity). We can ask what happens at the limit, i.e. $\lim_{t \to 0}\pi_t = \sup_{t>0}\pi_t < \pi_{-1}$ (note the limit is bounded, so exists).

\begin{theorem}
    $$\pi_0 = \lim_{t \to 0}\pi_t = \frac{1}{1+r}\mathbb{E}^{\mathbb{Q}}[Y]$$ where $\frac{d \mathbb{Q}}{d \mathbb{P}} \propto U'(X^*)$, i.e. it is proportional to the marginal utility of the time 1 optimal wealth $X^*$, i.e. the maximizer $\mathbb{E}[U(X)]$ over $X \in \mathcal{X}(x)$. 
\end{theorem}
\textbf{Remark.} This provides a \textbf{linear} pricing rule for a very small number of shares of the claim.

\textbf{Remark.} $\mathbb{Q}$ is a risk-neutral measure, so $\mathbb{E}^{\mathbb{Q}}[S_1] = (1+r)S_0$ (first order condition for a maximum). This means that for any $X \in \mathcal{X}(x)$, $$\mathbb{E}^{\mathbb{Q}}[X] = (1+r)x + \theta^\top\mathbb{E}^{\mathbb{Q}}[S_1-(1+r)S_0] = (1+r)x.$$

\begin{proof}
    Let $X_t$ be the maximizer of $$\mathbb{E}[U(X+t(Y-(1+r)\pi_t))]$$ over $X \in \mathcal{X}(x)$ and $t>0$. Then
    \begin{align*}
        0 = \frac{1}{t}\mathbb{E}\left[U(X_t+t(Y-(1+r)\pi_t)) - U(X_0)\right] \ge \\
        \frac{1}{t}\mathbb{E}[U(X_0+t(Y-(1+r)\pi_t)-U(X_0)]
    \end{align*}
    As $t \mapsto \pi_t$ is decreasing, so the above is 
    \begin{align*}
        \ge \mathbb{E}\left[\frac{U(X_0+t(Y-(1+r)\pi_0)-U(X_0))}{t}\right] \to \mathbb{E}[U'(X_0)](Y-(1+r)\pi_0).
    \end{align*}
    Hence $$\pi_0 \ge \frac{1}{1+r}\mathbb{E}^{\mathbb{Q}}[Y] = \frac{\mathbb{E}[U'(X_0)Y]}{(1+r)\mathbb{E}[U'(X_0)]}.$$

    \marginpar{21 Oct 2022, Lecture 7}
    
    For the reverse direction, we use the following lemma:
    \begin{lemma}
        For any $x,y$ we have \[
        U(y) \le U(x) + U'(x)(y-x).
        \]
    \end{lemma}
    \begin{proof}
        For $x<x+\epsilon<y$ we have $\frac{U(y)-U(x)}{y-x}\le \frac{U(x+\epsilon)-U(x)}{\epsilon} \stackrel{\epsilon \to 0}{\to} U'(x)$ by concavity $U((1-p)x+py) \ge (1-p)U(x)+py$ with $p=\frac{\epsilon}{y-x}$. The case $y<x$ is similar.
    \end{proof}

    Hence
    \begin{align*}
        0 = \frac{1}{t}\mathbb{E}[U(X_t+ t(Y-(1+r)\pi_t))-U(X_0)] \le\\
        \frac{1}{t}\mathbb{E}[U'(X_0)(X_t+t(Y-(1+r)\pi_0)-X_0)] = \\
        \mathbb{E}[U'(X_0)(Y-(1+r)\pi_t)]
    \end{align*}
    where we use the fact that $\mathbb{E}[U'(X_0)X]=(1+r)X$ for any $X \in \mathcal{X}(x)$, so in particular $\mathbb{E}[U'(X_0)(X_t-X_0)]=0$. Hence
    \[
    \pi_t \le \frac{\mathbb{E}[U'(X_0)Y]}{(1+r)\mathbb{E}[U'(X_0)]} = \frac{1}{1+r}\mathbb{E}^{\mathbb{Q}}[Y].
    \]
    Taking the limit as $t \to 0$ finishes (the case $t<0$ is similar).
\end{proof}

\subsection{Arbitrage}

Recall our setup: we have one risk-free asset with interest rate $r$, and $d$ risky assets with time $t$ price $S_t$.

\begin{defn}
    An \textbf{arbitrage} is a portfolio $\phi \in \mathbb{R}^d$ such that
    \begin{align*}
        &\phi^\top(S_1-(1+r)S_0) \ge 0 \text{ almost surely, and} \\
        &\mathbb{P}(\phi^\top(S_1-(1+r)S_0)>0)>0.    
    \end{align*}
\end{defn}

Fix some initial wealth $X_0=x$ and an increasing utility function $U$. Consider the problem of maximizing $\mathbb{E}[U(X_1)]$ over $X_1 \in \mathcal{X}(x)$, where $$\mathcal{X}(x) = \{(1+r)x + \theta^\top(S_1-(1+r)S_0) ~|~ \theta \in \mathbb{R}^d\}.$$
Suppose this has arbitrage $\phi$. Then, given any $X \in \mathcal{X}(x)$, consider $$X^* = X + \phi^\top(S_1-(1+r)S_0).$$ Note that $X^* \in \mathcal{X}(x)$, but also $U(X^*)\ge U(X)$ almost surely and $\mathbb{P}(U(X^*)>U(X))>0$, hence $\mathbb{E}[U(X^*)] > \mathbb{E}[U(X)]$. But $X$ was arbitrary, so there can be no maximizer!
\vspace{1mm}

Arbitrages are bad for our theory: if $\phi$ is an arbitrage, then by above, the portfolio $\theta + (n+1)\phi$ is better than $\theta+n \phi$ for all $n$. As $n$ gets large, the assumption that an agent can trade with no price impact becomes more and more unrealistic.

\textbf{Remarks.} The definition of arbitrage does not depend on the agent's initial wealth $x$ or utility function $U$. However, it does depend on the agent's beliefs through the probability measure $\mathbb{P}$. Nevertheless, if one agent believes $\phi$ is an arbitrage, then another agent with equivalent beliefs (i.e. agreeing on almost sure events) would also believe that $\phi$ is an arbitrage.

\subsubsection{Fundamental theorem of asset pricing}
So far, we know that:
\begin{itemize}
    \item If there exists an optimal solution to a utility maximization problem, then there exists a risk-neutral measure. 
    \item If there exists an optimal solution to a utility maximization problem, then there exists no arbitrage.
\end{itemize} 
    
\begin{theorem}[Fundamental theorem of asset pricing]
    A market model has no arbitrage if and only if there exists a risk-neutral measure.
\end{theorem}
\begin{proof}
    $\impliedby$: This is the easy direction. Let $\phi$ be a portfolio such that \[
    \mathbb{P}(\phi^\top(S_1-(1+r)S_0)\ge 0)=1.
    \]
    Suppose there exists a risk-neutral measure $\mathbb{Q}$. Then, by equivalence, we have $\mathbb{Q}(\phi^\top(S_1-(1+r)S_0)\ge 0)=1$. However, by the definition of risk-neutrality, we get $\mathbb{E}^\mathbb{Q}[\phi^\top(S_1-(1+r)S_0)]=\phi^\top \mathbb{E}^{\mathbb{Q}}[S_1-(1+r)S_0] = 0$.

    Hence, by the pigeonhole principle, $\mathbb{Q}(\phi^\top(S_1-(1+r)S_0)>0)=0$. By equivalence, $\mathbb{P}(\phi^\top(S_1-(1+r)S_0)>0)=0$, so $\phi$ is not an arbitrage and we're done.
    \vspace{1mm}
    
    \marginpar{24 Oct 2022, Lecture 8}
    
    $\implies$: Let $\xi = S_1 - (1+r)S_0$. We can assume WLOG that $\mathbb{E}[e^{-\theta^\top \xi}] < \infty$ for all $\theta \in \mathbb{R}^d$ by replacing $\mathbb{P}$ with an equivalent measure $\tilde{\mathbb{P}}$ with density \[
    \frac{d \tilde{\mathbb{P}}}{d \mathbb{P}} \propto e^{-|\xi|^2}
    \] 
    and noting that we have no $\mathbb{P}$--arbitrages iff we have no $\tilde{\mathbb{P}}$--arbitrages.
    \vspace{1mm}
    
    Motivating aside: the existence of an optimal solution to a utility maximization problem $\implies \exists$ a risk-neutral measure with density $\frac{d\mathbb{Q}}{d\mathbb{P}} \propto U'(X_1^*)$. End of aside.
    \vspace{1mm}
    
    Let us try to maximize $\mathbb{E}[U(\theta^\top \xi)]$, where $U(x)=-e^{-x}$. 
    
    Let $(\theta_n)_n$ be such that $\mathbb{E}[U(\theta^\top_n \xi)] \to \sup_{\theta \in \mathbb{R}^d} \mathbb{E}[U(\theta^\top \xi)]$. We consider two cases:
    \begin{itemize}
        \item $(\theta_n)_n$ is bounded. By Bolzano-Weierstrass, there exists a convergent subsequence, so we may assume that $(\theta_n) \to \theta_0$ for some portfolio $\theta_0$. But $$\mathbb{E}[U(\theta_0^\top \xi)] = \lim \mathbb{E}[U(\theta_0^\top \xi)] = \sup_{\theta \in \mathbb{R}^d} \mathbb{E}[U(\theta^\top \xi)]$$ (by continuity of $\theta \mapsto \mathbb{E}[e^{-\theta^\top\xi}]$). So $\theta_0$ is an optimal portfolio. Hence we're done, as the marginal utility at the optimal wealth, $U'(\theta_0^\top \xi),$ is proportional to the density of a risk-neutral measure.
        \item Every maximizing sequence $(\theta_n)_n$ is unbounded. Assume for contradiction that there is no arbitrage. We may assume WLOG that the components of $\xi$ are linearly independent, i.e. $(\mathbb{P}(\theta^\top \xi = 0)=1) \implies \theta = 0$. We may assume this since the original market has no arbitrage, so any submarket also has no arbitrage, so just pass to a submarket if need be.
        \vspace{1mm}
        
        We may assume that $|\theta_n| \to \infty$. Let $\varphi_n = \frac{\theta_n}{|\theta_n|}$. Note $(\varphi_n)_n$ is bounded, so by Bolzano-Weierstrass we may assume $\varphi_n \to \varphi_0$ for some $\varphi_0$.

        To finish the proof, we will now show that $\varphi_0^\top \xi \ge 0$ almost surely. Since we have no arbitrage, $\varphi_0^\top\xi=0$ almost surely. By linear independence, this means $\varphi_0 = 0$, contradicting $|\varphi_0|=1$.
        \vspace{1mm}
        
        To show $\varphi_0^\top \xi \ge 0$ almost surely, it is enough to show that $$\mathbb{P}(\varphi_0^\top \xi < -\epsilon, |\epsilon|<r)=0$$ for all $\epsilon,r>0$ (by continuity of probability measures).
        \vspace{1mm}
        
        Note on $\{\varphi_0^\top \xi < -\epsilon, |\xi|<r\}$, we have \[
        \varphi_n^\top \xi \le (\varphi_n-\varphi_0)^\top \xi + \varphi_0^\top \xi \le |\varphi_n-\varphi_0|r - \epsilon.
        \]
        Pick $N$ such that $|\varphi_n-\varphi_0|\le \frac{\epsilon}{2r}$ for $n\ge N$, so the above is $\le -\frac{\epsilon}{2}$.
        \vspace{1mm}
        
        Now, since $\theta=0$ is not optimal, we get
        \begin{align*}
            1 = - \mathbb{E}[U(0)] \ge - \mathbb{E}[U(\theta_n^\top \xi)] = \mathbb{E}[\left(e^{-\varphi_n \xi}\right)^{|\theta_n|}] \ge \\
            \mathbb{E}[\left(e^{-\varphi_n \xi}\right)^{|\theta_n|} \mathbbm{1}_{\{\varphi_0^\top \xi < -\epsilon, |\xi|<r\}}] \ge e^{\frac{\epsilon}{2}|\theta_n|}\mathbb{P}(\varphi_0^\top \xi < -\epsilon, |\xi| < r),
        \end{align*}
        so \[
        \mathbb{P}(\varphi_0^\top \xi < -\epsilon, |\xi|<r) \le e^{-\frac{\epsilon}{2}|\theta_n|} \to 0.
        \]
    \end{itemize}
\end{proof}
\textbf{Remark.} The details of the above proof are all examinable. The entire proof is a bit longer than the average bookwork question, so the emphasis should be on understanding each part of the proof separately.

\subsection{No-arbitrage pricing}
Given a market model with $r, S_0, S_1$, and the payout of a contingent claim, we want a time 0 price $\pi$ for the claim. So far we can:
\begin{itemize}
    \item Given $X_0=x, U$, calculate the indifference price.
    \item Given $X_0=x, U$, calculate the marginal utility price.
\end{itemize}
Now assume that our original market has no arbitrage, we want to find $\pi$ such that the new market also has no arbitrage.
\begin{theorem}
    If the original market has no arbitrage, then the augmented market also has no arbitrage if and only if $\pi = \frac{1}{1+r}\mathbb{E}^\mathbb{Q}[Y]$ for some risk-neutral measure in the original market.
\end{theorem}
\begin{proof}
    By FTAP applied to the original market, there exist risk-neutral measures $\mathbb{Q}$ such that $\mathbb{E}^\mathbb{Q}[S_1]=(1+r)S_0$. By FTAP applied to the augmented market, there is no arbitrage if and only if there exists a risk-neutral measure for the new market, i.e. $\mathbb{E}^\mathbb{Q}\left[\begin{pmatrix} S_1 \\ Y \end{pmatrix}\right] = (1+r)\begin{pmatrix} S_0 \\ \pi \end{pmatrix}$.
\end{proof}
\begin{cor}
    The set of no-arbitrage prices is an interval.
\end{cor}
\begin{proof}
    Let $I$ be the set of no-arbitrage prices for the claim. Suppose $\pi_0,\pi_1 \in I$. By FTAP, $\pi_0 = \frac{1}{1+r} \mathbb{E}^{\mathbb{Q}_0}[Y]$, $\pi_1 = \frac{1}{1+r}\mathbb{E}^{\mathbb{Q}_1}[Y]$. Let $p \in (0,1)$ and $\mathbb{Q}_p = p\mathbb{Q}_1 + (1-p)\mathbb{Q}_0$. Note that 
    \[
    \mathbb{E}^{\mathbb{Q}_p}[S_1] = p \mathbb{E}^{\mathbb{Q}_1}[S_1] + (1-p) \mathbb{E}^{Q_0}[S_1] = p(1+r)S_0 + (1-p)(1+r)S_0 = (1+r)S_0,
    \]
    so $\mathbb{Q}_p$ is risk-neutral. Hence $\pi_p = \frac{1}{1+r} \mathbb{E}^{\mathbb{Q}_p}[Y] \in I$ by FTAP, but $\pi_p = p \pi_1 + (1-p) \pi_0$, so $I$ is convex. 
\end{proof}

\subsection{Attainable claims}
\marginpar{26 Oct 2022, Lecture 9}
Consider putting $\theta^0$ in the bank and holding $\theta \in \mathbb{R}^d$ in risky assets. The time 1 value is $\theta^0(1+r) + \theta^\top S_1$.
\begin{defn}
    A claim is \textbf{attainable} if and only if its time 1 payout $Y$ is $Y = \theta^0(1+r) + \theta^\top S_1$ for some $\theta^0 \in \mathbb{R}, \theta \in \mathbb{R}^d$, i.e. it can be attained in our market.
\end{defn}
\textbf{Remark.} We could also write $Y=(1+r)y + \theta^\top(S_1-(1+r)S_0) \in \mathcal{X}(y)$ where $y - \theta^\top S_0 = \theta^0$.

On the example sheet we will show that there is a unique indifference price $\pi = \theta^0 + \theta^\top S_0 = y$ and it is independent of $X_0 = x$ or the utility function $U$. This means that there is a unique marginal utility price.

Now we will show that there is a unique no-arbitrage price.

\begin{theorem}
    Suppose the original market has no arbitrage.Then an attainable claim has a unique no-arbitrage price, i.e. if it is added to the market, then there is only one initial price such that the augmented market has no arbitrage.
\end{theorem}
\begin{proof}[Proof 1, primal proof]
    Suppose $Y = (1+r)y + \theta^\top(S_1-(1+r)S_0)$. Let $\pi$ be the initial price.
    \vspace{1mm}
    
    Claim: If $\pi=y$, then there is no arbitrage, and if $\pi \neq y$, then there exists arbitrage.

    Proof: Consider a portfolio $(\phi^\top, a)^\top \in \mathbb{R}^{d+1}$, a portfolio in the augmented market. Suppose $\phi^\top(S_1-(1+r)S_0) + a(Y-(1+r)y) \ge 0$ almost surely (in the case where $\pi = y$). But the above is \[
    (\phi^\top + a \theta)^\top(S_1-(1+r)S_0)\ge 0 \text{ a.s.}
    \]
    Since there is no arbitrage in the original market, $$(\phi+a \theta)^\top(S_1-(1+r)S_0)=0 \text{ a.s.}$$
    This implies that there is no arbitrage in the augmented market.
    \vspace{1mm}
    
    Now let us consider the case $\pi > y$. Our intuition is that we can buy it for price $y$ and sell it for price $\pi$, which is an arbitrage. 

    Consider $(\theta^\top,-1)^\top \in \mathbb{R}^d$. We have \[
    \theta^\top(S_1-(1+r)S_0)-(Y-(1+r)\pi) = (\pi-y)(1+r)>0,    
    \]
    so this is an arbitrage in the augmented market.

    The case $\pi < y$ is analogous, so we're done.
\end{proof}
\begin{proof}[Proof 2, "dual" proof]
    We know there is no arbitrage in the augmented market if and only if there exists a risk-neutral measure $\mathbb{Q}$ for the original market such that $$\pi = \frac{\mathbb{E}^{\mathbb{Q}}[Y]}{1+r} = \frac{\mathbb{E}^\mathbb{Q}[(1+r)y + \theta^\top(S_1-(1+r)S_0)]}{1+r} = y + \frac{\theta^\top}{1+r}\mathbb{E}^{\mathbb{Q}}[S_1-(1+r)S_0] \stackrel{\star}{=} y,$$
    where $\star$ follows from the definition of a risk-neutral measure.
\end{proof}
We also have a converse result:
\begin{theorem}
    If a claim has a unique no-arbitrage price, then it is attainable.    
\end{theorem}
\begin{proof}
    Exercise on example sheet 2 (it is a consequence of FTAP).
\end{proof}
\begin{example}
    A forward contract is the right and the obligation to buy a given asset at time 1 for a fixed amount $K$ (the strike price). Let $d=1$, so our payout is $Y=S_1 - K$. This is attainable by holding one share of the asset and borrowing $\frac{K}{1+r}$ from the bank. The initial no-arbitrage price is $\pi = S_0 - \frac{K}{1+r}$.
    \vspace{1mm}

    Aside: this is the concept of the forward price of an asset. Usually, we choose $K$ such that $\pi = 0$. This special $K=(1+r)S_0$ is called the forward price of an asset. End of aside.
\end{example}
\begin{example}
    Consider the one-period binomial model (that we talked about in lecture 5), where $S_0$ becomes either $S_0(1+b)$ or $S_0(1+a)$ at time 1 with probabilities $p$ and $1-p$. Let $Y=g(S_1)$. Turns out this is attainable for any $g$, since we can solve 
    \begin{align*}
        \theta^0(1+r) + \theta S_0 (1+b) &= g(S_0(1+b)) \\
        \theta^0(1+r) + \theta S_0 (1+a) &= g(S_0(1+a)) \\
        \implies \theta = \frac{g(S_0(1+b))-g(S_0(1+a))}{S_0(b-a)}&, \theta^0 = \frac{(1+b)g(S_0(1+a))-(1+a)g(S_0(1+b))}{(1+r)(b-a)}.
    \end{align*}
    The first of these equations will appear a lot in the upcoming lectures.

    Exercise: Check that $$\theta^0 + \theta S_0 = \frac{\mathbb{E}^{\mathbb{Q}}[g(S_1)]}{1+r},$$ where $$\mathbb{Q}(S_1=(1+b)S_0) = \frac{r-a}{b-a}, ~\mathbb{Q}(S_1=(1+a)S_0) = \frac{b-r}{b-a}.$$
\end{example}

\section{Multiperiod models}
\subsection{Introduction to multiperiod models}
In one period, $S_0$ is constant and $S_1$ is a random vector. In two periods, both $S_1$ and $S_2$ are random. At time 1, we will know $S_1$, but not $S_2$. So $S_2$ is "more random" in a sense.

We want to have three "sets of information" $\mathcal{F}_0, \mathcal{F}_1, \mathcal{F}_2$. $\mathcal{F}_0$ has no information, while $\mathcal{F}_2$ has full information (so we know what all the prices are). $\mathcal{F}_1$ is an intermediate situation, where we know $S_1$, but not $\mathcal{F}_2$.

We also have $\mathcal{F}_0 \subset \mathcal{F}_1 \subset \mathcal{F}_2$. These are filtrations, and we will talk about them in detail later.

What is information? Let $g$ be a "set of information". Intuitively, we have a notion of conditional probability of $A$ given $g$, $\mathbb{P}(A \mid g)$. So our intuitive definition is that an event $A$ is $\mathcal{G}$--measurable if and only if $\mathbb{P}(A \mid \mathcal{G}) \in \{0,1\}$ always.
\begin{example}
    Let $\Omega = \{HH,HT,TH,TT\}$, and $\mathcal{F}_1$ the information we get from observing the first coin. Then $A = \{HH,HT\}$ is $\mathcal{F}_1$--measureable, because $\mathbb{P}(A \mid \mathcal{G}) = \begin{cases}
        1 \text{ if first flip is heads.}\\
        0 \text{ if first flip is tails.}
    \end{cases}$

    But if $B = \{HT\}$, then $\mathbb{P}(B \mid \mathcal{G}) = \begin{cases}
        \frac{1}{2} \text{ if first flip is heads.}\\
        0 \text{ if first flip is tails.}
    \end{cases}$

    So $B$ is not $\mathcal{F}_1$--measurable.
\end{example}
Idea: the set of information can be identified with the collection of measurable events.

\subsection{Measurability}

\marginpar{28 Oct 2022, Lecture 10}

\begin{defn}
    Given a sample space $\Omega$ (a set of outcomes), a \textbf{sigma-algebra}  on $\Omega$ is a collection $\mathcal{G}$ of events (subsets of $\Omega$) such that
    \begin{itemize}
        \item it is nonempty;
        \item if $A \in \mathcal{G}$, then $A^c \in \mathcal{G}$;
        \item if $A_1,A_2,\ldots \in \mathcal{G}$, then $\bigcup_{n} A_n \in \mathcal{G}$.\footnote{If it is nonempty, then some set $A$ and its complement $A^C$ are in the set, so their union, i.e. the empty set,    and its complement $\Omega$ are in our set as well.}
    \end{itemize} 
\end{defn}
\begin{example}
    If $\Omega = \{HH,HT,TH,TT\}$, then $\mathcal{G} = \{\emptyset, \Omega, \{HH,HT\}, \{TH,TT\}\}$ models the information from the first coin toss.
\end{example}
\begin{defn}
    Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ (the sample space, the collection of all events, and a probability measure) and a sub $\sigma$--algebra $\mathcal{G} \subset \mathcal{F}$, a random variable $X$ is \textbf{$\mathcal{G}$--measurable}  if and only if $$\{X \le x\} \in \mathcal{G} ~\forall x \in \mathbb{R}.$$
\end{defn}
So intuitively, knowing the information in $\mathcal{G}$ allows you to measure the value of $X$.

\textbf{Remark.} If $x$ is $\mathcal{G}$--measurable, then $\{x \in B\} \in \mathcal{G}$ for any ''nice''\footnote{Here, ''nice'' means a Borel set.} $B \subset \mathbb{R}$.

\textbf{Remark.} If $X$ is discrete, taking values in $\{x_1,x_2,\ldots,\}$, then $X$ is $\mathcal{G}$--measurable if and only if $\{X = x_i\} \in \mathcal{G} ~\forall i$.

\textbf{Exercise.} Let $\mathcal{G} = \{\emptyset, \Omega\}$. Show that if $X$ is $\mathcal{G}$--measurable, then $X$ is a constant.
\begin{defn}
    The \textbf{sigma-algebra generated by a random variable}  $X$ is the collection of events $\{X \in B\}$ for all ''nice'' $B \subset \mathbb{R}$. We denote this as $\sigma(X)$.
\end{defn}
\begin{theorem}
    A random variable $Y$ is $\sigma(X)$--measurable if and only if we have $Y=f(X)$ for some ''nice'' function $f : \mathbb{R} \to \mathbb{R}$.
\end{theorem}

Recall the conditional expectation given an event: Let $X$ be integrable and $G \in \mathcal{F}$ such that $\mathbb{P}(G)>0$, then $\mathbb{E}[X \mid G]$ is defined to be\[
\mathbb{E}[X \mid G] = \frac{\mathbb{E}[X \mathbbm{1}_{G}]}{\mathbb{P}(G)}.
\]
Recall also: Let $Y$ take values in $\{y_1,y_2,\ldots\}$ with $\mathbb{P}(Y=y_i)>0 ~\forall i$. Then $\mathbb{E}[X \mid Y]$ is defined as $f(Y)$, where \[
f(y_i) = \mathbb{E}[X \mid Y=y_i].
\]
Note that $\mathbb{E}[X \mid Y]$ is $\sigma(Y)$--measurable (as it is a function of $Y$).

We also have a projection property: $$\mathbb{E}[\mathbb{E}[X \mid Y]Z]=\mathbb{E}[XZ]$$ for any bounded $\sigma(Y)$--measurable $Z$.

\begin{proof} We know that $\exists g$ such that $Z=g(Y)$ (by measurability). Then
    \begin{align*}
        \text{LHS}=\mathbb{E}[f(Y)Z]=\mathbb{E}[f(Y)g(Y)] = \sum_{i}^{} \mathbb{P}(Y=y_i)f(y_i)g(y_i) = \\
        \sum_{i}^{} P(Y=y_i)\mathbb{E}[X \mid Y=y_i]g(y_i) = \sum_{i}^{} \mathbb{E}[X\mathbbm{1}_{\{Y=y_i\}}g(y_i)] = \\
        \mathbb{E}\left[X(\sum_{}^{} \mathbbm{1}_{\{Y=y_i\}}g(y_i))\right] = \mathbb{E}[Xg(Y)] = \mathbb{E}[XZ] = \text{RHS}.
    \end{align*}
\end{proof}
\begin{defn}
    Let $X$ be defined on $(\Omega, \mathcal{F}, \mathbb{P})$, suppose $X$ is integrable (i.e. $\mathbb{E}[|X|]<\infty$) and let $\mathcal{G} \subset \mathcal{F}$ be a sub $\sigma$--algebra. \textbf{The conditional expectation of $X$ given $\mathcal{G}$}  (written as $\mathbb{E}[X \mid \mathcal{G}]$) is any $\mathcal{G}$--measurable integrable random variable $Y$ such that $$\mathbb{E}[X \mathbbm{1}_G] = \mathbb{E}[Y \mathbbm{1}_G]$$ for all $G \in \mathcal{G}.$
\end{defn}
\begin{theorem}
    Conditional expectations are almost surely unique (i.e. if $Y_1$ and $Y_2$ are conditional expectations of the same thing, then $\mathbb{P}(Y_1=Y_2)=1$).
\end{theorem}
\begin{proof}
    We have $\mathbb{E}[X \mathbbm{1}_G] = \mathbb{E}[Y_1 \mathbbm{1}_G] = \mathbb{E}[Y_2 \mathbbm{1}_G]$ for any $G \in \mathcal{G}$. It follows that $\mathbb{E}[(Y_1-Y_2) \mathbbm{1}_G]= 0$. Let $G=\{Y_1 > Y_2\}$. This is $\mathcal{G}$--measurable, since both $Y_1$ and $Y_2$ are $\mathcal{G}$--measurable. By pigeonhole, $\mathbb{P}(Y_1 > Y_2) = 0$, and by symmetry, $\mathbb{P}(Y_1 < Y_2) = 0$.
\end{proof}
\textbf{Remark.} $\mathbb{E}[XZ] = \mathbb{E}[\mathbb{E}[X \mid \mathcal{G}]Z]$ for any bounded $\mathcal{G}$--measurable $Z$. This is proved using measure theory.
\begin{theorem}
    Suppose $X$ is square--integrable (i.e. $\mathbb{E}[X^2]<\infty$). Then $$\mathbb{E}[(X-Y)^2] \ge \mathbb{E}[(X-\mathbb{E}[X \mid \mathcal{G}])^2]$$ for any $\mathcal{G}$--measurable $Y$.
\end{theorem}
\begin{proof}[Sketch of proof]
    Given an arbitrary $\mathcal{G}$--measurable random variable $Y$, let $Z = Y - \mathbb{E}[X \mid \mathcal{G}]$. Then
    \begin{align*}
        \mathbb{E}[(X-Y)^2] = \mathbb{E}[(X-\mathbb{E}[X \mid \mathcal{G}] + Z)^2] = \\ 
        \mathbb{E}[(X-\mathbb{E}[X \mid \mathcal{G}]^2) + 2\mathbb{E}[Z(X-\mathbb{E}[X \mid \mathcal{G}])]] + \mathbb{E}[Z^2] = \\
        \mathbb{E}[(X-\mathbb{E}[X \mid \mathcal{G}]^2) + \mathbb{E}[Z^2] \ge \mathbb{E}[(X- \mathbb{E}[X \mid  \mathcal{G}])^2].
    \end{align*}
\end{proof}

\subsubsection{Properties of conditional expectations} 

\marginpar{31 Oct 2022, Lecture 11}

\begin{example}
    Suppose $\Omega = \bigcup_{n} G_n$ for $(G_n)_n$ disjoint and $\mathbb{P}(G_n)>0 ~\forall n$. Let ${\mathcal{G} = \{\bigcup_{n \in I} G_n \mid I \subset \mathbb{N}\}}$ and let $X$ be integrable. Then \[
    \mathbb{E}[X \mid \mathcal{G}](\omega) = \mathbb{E}[X \mid G_n], ~\omega \in G_n.
    \]
\end{example}
\textbf{Remark.} Let $Y(\omega) = n$ if $ \omega \in G_n$. Note that $\mathcal{G} = \sigma(Y)$ and $\mathbb{E}[X \mid \mathcal{G}] = \mathbb{E}[X \mid Y]$.
\vspace{1mm}

\textbf{Notation.} We have by definition $\mathbb{E}[X \mid Y] = \mathbb{E}[X \mid \sigma(Y)]$.

\begin{theorem}
    Suppose all conditional expectations are defined. We then have:
    \begin{itemize}
        \item Additivity: $\mathbb{E}[X+Y \mid \mathcal{G}] = \mathbb{E}[X \mid \mathcal{G}] + \mathbb{E}[Y \mid \mathcal{G}]$.
        \item Pulling out a known factor: Suppose $Y$ is $\mathcal{G}$--measurable, then $$\mathbb{E}[XY \mid \mathcal{G}] = Y \mathbb{E}[X \mid \mathcal{G}].$$
        \item Tower property: if $\mathcal{H} \subseteq \mathcal{G}$, then \[
        \mathbb{E}[\mathbb{E}[X \mid \mathcal{G}] \mid \mathcal{H}] = \mathbb{E}[\mathbb{E}[X \mid \mathcal{H}] \mid \mathcal{G}] = \mathbb{E}[X \mid \mathcal{H}].
        \]
        A corollary: $\mathbb{E}[\mathbb{E}[X \mid \mathcal{G}]] = \mathbb{E}[X]$ (by letting $\mathcal{H}= \{\emptyset,\Omega\}$).
        \item If $X$ is independent of $\mathcal{G}$ (so $\{X \in B\}$ is independent of $G$ for all nice $B$ and $G \in \mathcal{G}$, or even $\mathbb{P}(\{X \in B\} \cap G) = \mathbb{P}(X \in B)\mathbb{P}(G)$), then \[
        \mathbb{E}[X \mid \mathcal{G}] = \mathbb{E}[X].
        \]
        \item Positivity: If $X\ge 0$ almost surely, then $\mathbb{E}[X \mid \mathcal{G}]\ge 0$ almost surely.
        \item Jensen's inequality: If $f$ is convex, then $\mathbb{E}[f(X) \mid \mathcal{G}] \ge f(\mathbb{E}[X \mid \mathcal{G}])$ almost surely.
    \end{itemize}
\end{theorem}
\begin{proof}[Sketch of proof]
    The first four: use the definition/uniqueness of conditional expectation. Positivity: use the definition again with $G = \{\mathbb{E}[X \mid G]\le 0\}$.

    Jensen: Suppose $f$ is differentiable. Then we have the supporting hyperplane theorem $f(y)\ge f(x)+f'(x)(y-x)$. Set $y=X, x= \mathbb{E}[X \mid \mathcal{G}]$ to get 
    \begin{align*}
        &f(X) \ge f(\mathbb{E}[X \mid \mathcal{G}]) + f'(\mathbb{E}[X \mid \mathcal{G}])(X-\mathbb{E}[X\mid \mathcal{G}]) \\
        \implies \mathbb{E}[f(X) \mid \mathcal{G}] \ge& \mathbb{E}[f(\mathbb{E}[X \mid \mathcal{G}])\mid  \mathcal{G}] + \mathbb{E}[f'(\mathbb{E}[X \mid \mathcal{G}])(X-\mathbb{E}[X \mid \mathcal{G}]) \mid G] = \\
        = f(\mathbb{E}[X \mid \mathcal{G}]) &+ f'(\mathbb{E}[X \mid G])\mathbb{E}[X - \mathbb{E}[X \mid G] \mid G] = f(\mathbb{E}[X \mid G]).     
    \end{align*}
\end{proof}
\begin{defn}
    $\mathbb{P}(A \mid \mathcal{G})=\mathbb{E}[\mathbbm{1}_A \mid \mathcal{G}]$.
\end{defn}
\textbf{Remark.} Suppose $\mathbb{P}(A \mid \mathcal{G}) \in \{0,1\}$ almost surely. Then it is an indicator function, so $\mathbb{P}(A \mid \mathcal{G}) = \mathbbm{1}_B$ for some $B$ that is $\mathcal{G}$--measurable.

Thus $\mathbb{P}(A)= \mathbb{E}[\mathbb{P}(A \mid \mathcal{G})] = \mathbb{E}[1_B]=\mathbb{P}(B)$ and $\mathbb{P}(A \cap B) = \mathbb{E}[\mathbb{E}[\mathbbm{1}_A \mathbbm{1}_B \mid \mathcal{G}]] = \mathbb{E}[\mathbbm{1}_B \mathbb{P}(A \mid G)] = \mathbb{E}[1_B] = \mathbb{P}(B)$. So now 
\[
\mathbb{E}[(\mathbbm{1}_A - \mathbbm{1}_B)^2] = \mathbb{E}[\mathbbm{1}_A] - 2\mathbb{E}[\mathbbm{1}_A \mathbbm{1}_B] + \mathbb{E}[\mathbbm{1}_B] = \mathbb{P}(A) - 2 \mathbb{P}(A \cap B) + \mathbb{P}(B) = 0,
\]
so $\mathbbm{1}_A = \mathbbm{1}_B$ almost surely, so $A$ is almost $B$--measurable.

\subsection{Adaptedness, filtrations, martingales}

\begin{defn}
    A \textbf{filtration} $(\mathcal{F}_n)_{n\ge 0}$ is a sequence of $\sigma$--algebras such that $\mathcal{F}_n \subseteq \mathcal{F}_{n+1} ~\forall n$.
\end{defn}
\textbf{Convention.} For us, $\mathcal{F}_0$ will always be the trivial $\sigma$--algebra $\{\emptyset, \Omega\}$. That means that $\mathcal{F}_0$--measurable variables are constants.

\begin{defn}
    A \textbf{(stochastic) process} $(X_n)_{n\ge 0}$ is a sequence of random variables.
\end{defn}
\begin{defn}
    A process $(X_n)_{n\ge 0}$ is \textbf{adapted} to a filtration $(\mathcal{F}_n)_{n\ge 0}$ if and only if $X_n$ is $\mathcal{F}_n$--measurable for all $n$.
\end{defn}
\textbf{Remark.} By convention, $X_0$ is constant (not random).

\begin{defn}
    A \textbf{martingale} with respect to a filtration $(\mathcal{F}_n)_n$ is an integrable process $(X_n)_{n\ge 0}$ such that $$\mathbb{E}[(X_n \mid \mathcal{F}_{n-1})] = X_{n-1}$$ almost surely for all $n\ge 1$.
\end{defn}
\textbf{Remark.} Alternatively, a martingale is an integrable process that's adapted and $\mathbb{E}[X_n - X_{n-1} \mid \mathcal{F}_n] = 0 ~\forall n\ge 1$.

\begin{example}
    Flipping a coin twice. $\Omega = \{HH,HT,TH,TT\}$. Let 
    \begin{align*}
        &\mathcal{F}_0 = \{\emptyset, \Omega\}, ~\mathcal{F}_1 = \{\emptyset, \Omega, \{HH,HT\}, \{TH,TT\}\},~ \mathcal{F}_2 = 2^{\Omega}.
    \end{align*}
    Being a martingale is a lot more restrictive then being adapted: $(X_n)_n$ is adapted (where $X_n$ is the result of the $n^{\text{th}}$ toss), and hence can take 7 values. On the other hand, a martingale $(m_n)_n$ has a lot of structure: $m_1 = \mathbb{E}[m_2 \mid \mathcal{F}_1]$, so assuming $\mathbb{P}(\omega)=\frac{1}{4} ~\forall \omega \in \Omega$, if the four outcomes have values $a,b,c,d$, then the two outcomes in $m_1$ are $\frac{a+b}{2}$ and $\frac{c+d}{2}$, and $m_0$ is $\frac{a+b+c+d}{4}$. So if we have a finite time horizon and we know the final values, then we have completely characterized the martingale, as we can just project backwards.
\end{example}

\marginpar{02 Nov 2022, Lecture 12}

Aside on the name ''risk--neutral measure''. Recall $U$ was increasing and concave, which means models prefer more to less, but are risk-averse. If $U''=0$, then the agent is said to be risk-neutral. Of course, in this case $U(x)=a+bx$ for $b>0$.

Let $Y$ be the payout of a claim and suppose there is no market. How much should a risk-neutral agent pay for $Y$? The indifference price solves \[
\mathbb{E}[U((1+r)x+Y-(1+r)\pi)] = U((1+r)x),
\]
which gives $\pi = \frac{1}{1+r}\mathbb{E}[Y]$. A risk-neutral measure is a fictional measure equivalent to the real measure (i.e. we reweight the probabilities of events) such that the given asset prices agree with the prices of a risk-neutral investor, i.e. exactly $S_0 = \frac{1}{1+r}\mathbb{E}^{\mathbb{Q}}[S_1]$. End of aside.

\subsubsection{Motivation for martingales in finance}
\begin{defn}
    A probability measure $\mathbb{Q}$ equivalent to $\mathbb{P}$ such that \[
    \mathbb{E}^{\mathbb{Q}}[S_n \mid \mathcal{F}_{n-1}] = (1+r)S_{n-1}
    \] for all $n\ge 1$ is called \textbf{risk--neutral} where $S_n$ is the price vector at time $n$.
\end{defn}
\textbf{Note.} $\mathbb{Q}$ is risk--neutral if and only if $\left(\frac{S_n}{(1+r)^n}\right)$ (the discounted prices) is a $\mathbb{Q}$--martingale. This is since 
\begin{align*}
    \mathbb{E}\left[\frac{S_n}{(1+r)^n} \mid \mathcal{F}_{n-1}\right] = \frac{S_{n-1}}{(1+r)^{n-1}} \iff \mathbb{E}[S_n \mid \mathcal{F}_{n-1}] = (1+r)S_{n-1}.
\end{align*}
\textbf{Remark.} Some people call a risk--neutral measure an equivalent martingale measure (EMM).
\begin{defn}
    The \textbf{filtration generated by a process}  $(X_n)_{n\ge 0}$ is $$\mathcal{F}_n = \sigma(X_0,\ldots,X_n).$$
    This is the smallest $\sigma$--algebra such that $(X_n)_n$ is adapted.
\end{defn}

\textbf{Examples of martingales.}

\begin{example}
    Let $X_1,X_2,\ldots$ be independent with $\mathbb{E}[X_n]=0 ~\forall n$ and $X_n$ integrable for all $n$. Let $S_0=0$ and $S_n = X_1 + \ldots + X_n$. Let $(\mathcal{F}_n)$ be generated by $(X_n)_{n\ge 1}$ where $\mathcal{F}_0 = \{\emptyset, \Omega\}$. The claim is that $(S_n)_{n\ge 0}$ is a martingale.
\end{example}
\begin{proof}
    We can check that $S_n$ is integrable, as $\mathbb{E}[|S_n|]\le \mathbb{E}[|X_1| + \ldots + |X_n|] < \infty$ since each term is finite. $S_n$ is $\mathcal{F}_n$--measurable (it is a function of $X_1,\ldots,X_n$), i.e $(S_n)_{n\ge 0}$ is adapted. As $X_n$ is independent of $\mathcal{F}_{n-1}$,
    \[
    \mathbb{E}[S_n - S_{n-1} \mid \mathcal{F}_{n-1}] = \mathbb{E}[X_n \mid \mathcal{F}_{n-1}] = \mathbb{E}[X_n] = 0.
    \] 
\end{proof}
\begin{example}
    Given an integrable random variable $X$ and a filtration $(\mathcal{F}_n)_{n\ge 0}$, let $Z_n = \mathbb{E}[X \mid \mathcal{F}_n]$. The claim is that $Z_n$ is a martingale.
\end{example}
\begin{proof}
    $Z_n$ is integrable as conditional expectation is integrable by definition.
    \[
    \mathbb{E}[Z_n \mid \mathcal{F}_{n-1}] = \mathbb{E}[\mathbb{E}[X \mid \mathcal{F}_n] \mid \mathcal{F}_{n-1}] \stackrel{(\star)}{=}  \mathbb{E}[X \mid \mathcal{F}_{n-1}] = Z_{n-1}
    \]
    where $(\star)$ follows from the tower property.
\end{proof}

\textbf{Useful fact}: To check that a given process $(M_n)_{0\le n \le N}$ on a finite horizon is a martingale, it is enough to check that 
\[
\mathbb{E}[M_N \mid \mathcal{F}_n] = M_n ~\forall n\le N.
\]
This is sometimes easier than the equivalent 
\[
\mathbb{E}[M_n \mid \mathcal{F}_{n-1}] = M_{n-1} ~\forall 1\le n\le N.
\]
\subsubsection{Martingale transforms}
\begin{defn}
    A process $(H_n)_{n\ge 1}$ is \textbf{previsible} (or \textbf{predictable}) with respect to $(\mathcal{F}_n)_{n\ge 0}$ if and only if $H_n$ is $\mathcal{F}_{n-1}$--measurable $~\forall n\ge 1$.
\end{defn}
Let $K_n = H_{n+1}$. Then $(K_n)_{n\ge 0}$ is adapted $\iff (H_n)_{n\ge 1}$ is previsible.

\begin{defn}
    Given a previsible $(H_n)_{n\ge 1}$ and adapted $(X_n)_{n\ge 0}$, the process defined as \[
    M_n = \sum_{k=1}^{n} H_k(X_k-X_{k-1})
    \]
    is the \textbf{martingale transform} of $(H_n)_n$ with respect to $(X_n)_n$.
\end{defn}
\begin{theorem}
    If $(H_n)_n$ is bounded and previsible and $(X_n)_n$ is a martingale, the martingale transform $(M_n)_{n\ge 0}$ is a martingale.
\end{theorem}
\begin{proof}
    $M_n$ is integrable, as $$\mathbb{E}[|M_n|]\le \mathbb{E}[\sum_{k=1}^{n} \underbrace{|H_k|}_{\text{bounded}}\underbrace{|X_k-X_{k-1}|}_{\text{integrable}}]$$ 
    and we have a finite number of finite terms.

    $M_n$ is also $\mathcal{F}_n$--measurable as it is a function of $H_1,\ldots,H_n$ and $X_0,\ldots,X_n$ which are all $\mathcal{F}_n$--measurable. Finally, 
    \[
    \mathbb{E}[M_n - M_{n-1} \mid \mathcal{F}_{n-1}] = \mathbb{E}[H_n(X_n-X_{n-1}) \mid \mathcal{F}_{n-1}] \stackrel{(\star)}{=}  H_n \underbrace{\mathbb{E}[X_n-X_{n-1} \mid \mathcal{F}_{n-1}]}_{=0 \text{ by def. of a martingale}}
    \]
    where $(\star)$ follows as $H_n$ is $\mathcal{F}_{n-1}$--measurable.
\end{proof}

\textbf{Martingale transforms in finance.} 

Let $(S_n)_n$ be the asset price vectors and $(\theta_n)_n$ the portfolios, with $\theta_n$ being $\mathcal{F}_{n-1}$--measurable (since it is the portfolio we hold between times $n-1$ and $n$) and let $X_n$ be our wealth at time $n$. Then
\begin{align*}
    X_n = (1+r)X_{n-1} &+ \theta^\top_n(S_n -(1+r)S_{n-1}) \\ &\iff\\  \frac{X_n}{(1+r)^{n}} = X_0 &+ \sum_{k=1}^{n} \theta_k^\top\left(\frac{S_k}{(1+r)^k} - \frac{S_{k-1}}{(1+r)^{k-1}}\right).    
\end{align*}

So an investor's discounted wealth is the martingale transform of his portfolio with respect to the discounted risk asset prices.

\marginpar{04 Nov 2022, Lecture 13}

\subsection{Stopping times}
\begin{defn}
    Given a filtration $(\mathcal{F}_n)_{n\ge 0}$, a stopping time is a random variable $T$ taking values in $\{0,1,2,\ldots\} \cup \infty$ such that $\{T\le n\} \in \mathcal{F}_n ~\forall n\ge 0$. 
\end{defn}
\begin{example}
    Let $(X_n)_n$ be an adapted process. Let $T = \inf \{n\ge 0 \mid X_n>0\}$ (with the convention that $\inf \emptyset = \infty$). Then the claim is that $T$ is a stopping time. Indeed, $\{T \le n\} = \bigcup_{k=0}^n \{X_k > 0\}$ and $\{X_k >0\} \in \mathcal{F}_k \subset \mathcal{F}_n$ for all $k\le n$ by adaptedness. Since $\mathcal{F}_n$ is closed under finite unions, we're done.
\end{example}

\textbf{Counterexample:} Let $(X_n)_{n\ge 0}$ be adapted and $T = \sup \{ n\ge 0 \mid X_n>0\}$. Then $T$ is not a stopping time in general.

\begin{prop}
    $T$ is a stopping time if and only if $\{T=n\} \in \mathcal{F}_n ~\forall n\ge 0$.
\end{prop}
\begin{proof}
    If $\{T=n\} \in \mathcal{F}_n ~\forall n$, then $\{T\le n\} = \bigcup_{k=0}^n \{T=k\} \in \mathcal{F}_n$. Conversely, if $T$ is a stopping time, then $\{T=n\} = \{T\le n\} \cap \{T\le n-1\}^{c} \in \mathcal{F}_n$.
\end{proof}

\begin{defn}
    Given an adapted process $(X_n)_{n\ge 0}$ and a stopping time $T$, \textbf{the process stopped at $T$} is the process \[
    X_n^T = X_{n \wedge T},
    \] 
    where $a \wedge b = \min(a,b)$.
\end{defn}

\begin{prop}
    $(X_n^T)_{n\ge 0}$ is the martingale transform of $(\mathbbm{1}_{\{n\le T\}})_{n\ge 1}$ with respect to $(X_n)_{n\ge 0}$.
    \begin{proof}
        We can check that $(\mathbbm{1}_{\{n\le T\}})_{n\ge 1}$ is previsible:
    \vspace{1mm}

    $\mathbbm{1}_{\{n \le T\}}$ is $\mathcal{F}_{n-1}$--measurable $\iff$ $\{n\le T\}\in \mathcal{F}_{n-1} \iff \{T\le n-1\}^{c} \in \mathcal{F}_{n-1}$ which is true, so we're done as $\sigma$--algebras are closed under complements. 
    \vspace{1mm}
    
    Now check
    \[
    X_{n \wedge T} = X_0 + \sum_{k=1}^{n} \mathbbm{1}_{\{k\le T\}}(X_k-X_{k-1}).
    \]
    \end{proof}
    
\end{prop}
\begin{theorem}
    A stopped martingale is a martingale.
\end{theorem}
\begin{proof}
    The martingale transform of a bounded previsible process with respect to a martingale is again a martingale, and $(\mathbbm{1}_{\{n\le T\}})_{n\ge 1}$ is bounded.
\end{proof}
\begin{theorem}[Optional sampling theorem]
    Let $(X_n)_{n\ge 0}$ be a martingale and let $T$ be a bounded stopping time (i.e. $\exists c$ a constant such that $T\le c$ almost surely). Then
    \[
    \mathbb{E}[X_T]=X_0.
    \]
\end{theorem}
\textbf{Remark.} Recall that under our conventions $X_0$ is a constant.
\begin{proof}
    Suppose that $T\le N$ almost surely. If $N$ is not random, then $X_T=X_{T \wedge N}$. But if $M_n=X_{n \wedge T}$, then $(M_n)_{n\ge 0}$ is a martingale. Now \[
    \mathbb{E}[X_T]=\mathbb{E}[M_n] = \mathbb{E}[\mathbb{E}[M_n \mid \mathcal{F}_{n-1}]]=\mathbb{E}[M_{n-1}],
    \]
    so by induction $\mathbb{E}[M_n]=M_0$ and we're done.
\end{proof}

Aside on limit theorems in probability:
\begin{theorem}[Monotone convergence theorem]
    Suppose $0\le X_1\le X_2\le X_3\le \ldots$ and let $Z=\sup_n X_n = \lim_{n \to \infty} X_n$. Then $\mathbb{E}[Z]=\lim_{n \to \infty} \mathbb{E}[X_n]$. In other words,
    \[
    \mathbb{E}[\lim_{n \to \infty}X_n] = \lim_{n \to \infty}\mathbb{E}[X_n].
    \]
\end{theorem}
\begin{theorem}[Dominated convergence theorem]
    Suppose $X_n \to Z$ almost surely and there exists an integrable $Y$ such that $|X_n|\le Y$. Then $\mathbb{E}[Z]=\lim_{n \to \infty}\mathbb{E}[X_n]$.
\end{theorem}
End of aside.
\begin{theorem}[Optional stopping theorem]
    Let $(X_n)_{n\ge 0}$ be a martingale. Let $T$ be a stopping time and suppose that $|X_{n \wedge T}|\le Y ~\forall n$ and $\mathbb{E}[Y]<\infty$. Then $\mathbb{E}[X_T]=X_0$.
\end{theorem}
\begin{proof}
    By optional sampling theorem, $\mathbb{E}[X_{n \wedge T}] = X_0$. By dominated convergence theorem, we're done since $X_{n \wedge T} \to X_T$ almost surely.
\end{proof}

\begin{example}
    Let $(X_n)_n$ be a simple symmetric random walk, so $X_n = \xi_1+\ldots+\xi_n$ for $(\xi_i)_i$ i.i.d. with $\mathbb{P}(\xi_i= \pm 1)=\frac{1}{2}$. Then $(X_n)_n$ is a martingale. 
    \vspace{1mm}
    
    Fix integers $a,b>0$. Let $T = \inf \{n\ge 0 \mid X_n \in \{-a,b\}\}$. We have $|X_{n \wedge T}|\le \max(a,b)$, so $\mathbb{E}[X_T]=0$ by optional stopping theorem. Hence we arrive at $-a \mathbb{P}(X_T=-a)+b\mathbb{P}(X_T=b)=0$, so $\mathbb{P}(X_T=-a)=\frac{b}{a+b}$ and $\mathbb{P}(X_T=b)=\frac{a}{a+b}$.
\end{example}

\textbf{Non-example.} Let $(X_n)_n$ be a simple symmetric random walk and $T= \inf \{n\ge 0 \mid X_n=-a\}$. Note that $X_T=-a$ almost surely, so $\mathbb{E}[X_T]=-a \neq 0$, seemingly a contradiction to optional stopping theorem. But $X_{n \wedge T}$ is not bounded from above, so we can't apply the optional stopping theorem.

\marginpar{07 Nov 2022, Lecture 14}

\begin{example}
    Let $X_n = \xi_1+\ldots \xi_n$ be a simple symmetric random walk with $\mathbb{P}(\xi = \pm 1)=\frac{1}{2}$. Fix $a>0$ an integer and let $T= \inf \{n\ge 0 \mid X_n=-a\}$. Our goal is to compute $\mathbb{E}[z^T]$ for $z \in (0,1)$, i.e. the probability generating function.
    \vspace{1mm}
    
    \textbf{Claim.} There exists $w$ such that $M_n = w^{X_n}z^n$ is a martingale (in the filtration generated by the process itself, i.e. $(X_n)$, or equivalently $(\xi)_n$).
    \vspace{1mm}
    
    \textbf{Proof.} We have
    $$\mathbb{E}[w^{X_n}z^n \mid \mathcal{F}_{n-1}] = z^n \mathbb{E}[w^{X_{n-1}+\xi_n} \mid \mathcal{F}_{n-1}] = z^n w^{X_{n-1}}\mathbb{E}[w^{\xi_n}]$$
    which equals $z^{n-1}w^{X_{n-1}}$ if and only if $z(\frac{1}{2}w + \frac{1}{2}w^{-1})=1$, which we can solve to get $w = \frac{1 \pm \sqrt{1-z^2}}{z}$.
    \vspace{1mm}
    
    Now apply optional stopping theorem: $\mathbb{E}[M_T]=M_0=1$ and $\mathbb{E}[w^{X_T}z^T]=w^{-a}\mathbb{E}[Z^T]=1$, so $\mathbb{E}[z^T]=w^a$. But which value of $w$ do we choose?
    \vspace{1mm}
    
    Note that $X_{T \wedge n}\ge -a$, so if $w<1$, then $w^{X_{t\wedge n}} \le w^{-a}$. So $M_n = w^{X_{T\wedge n}}z^{T \wedge n}\le w^{-a}$, so it is bounded uniformly in $n$. So OST is applicable if we choose $w=\frac{1-\sqrt{1-z^2}}{z}.$
\end{example}
\begin{example}
    Let $(X_n)_n$ be a SSRW again, and let $T=\inf \{n\ge 0 \mid X_n \in \{b,-a\}\}$. Fix $z \in (0,1)$. Our goal is to find $\mathbb{E}[z^T]$. We know that $M_n = w^{X_n}z^n$ is a martingale, where $w+w^{-1}=\frac{2}{z}$ (by the last example).
    \vspace{1mm}
    
    The trick is to define $M_n = w^{X_n-(\frac{b-a}{2})}z^n + w^{\frac{b-a}{2}-X_n}z^n$, which is also a martingale. Note $X_n - \frac{a+b}{2}= \frac{b+a}{2}$ or $\frac{-(b+a)}{2}$. Hence $M_T = (w^{\frac{a+b}{2}} + w^{\frac{-(a+b)}{2}})z^T$ (and $M_0=2$). Thus \[
        \mathbb{E}[z^T] = \frac{2}{w^{\frac{a+b}{2}}+w^{-\frac{(a+b)}{2}}},
    \]
    where $z=\frac{2}{w+w^{-1}}$. Note that $M_{n \wedge T}$ is bounded for either choice of $w$, and the choice of $w$ doesn't matter since the final formula is symmetric in $w$ and $w^{-1}$. 
\end{example}
\subsection{Submartingales and supermartingales}
\begin{defn}
    A \textbf{submartingale} is an integrable adapted process $(X_n)_{n\ge 0}$ such that \[
    \mathbb{E}[X_n \mid \mathcal{F}_{n-1}] \ge X_{n-1} \text{ a.s. }\forall n\ge 1.
    \]
\end{defn}
\begin{defn}
    A \textbf{supermartingale} is an integrable adapted process $(X_n)_{n\ge 0}$ such that \[
        \mathbb{E}[X_n \mid \mathcal{F}_{n-1}] \le X_{n-1} \text{ a.s. }\forall n\ge 1.
    \]
\end{defn}
\begin{prop}
    If $n<N$ and $(X_n)_{n\ge 0}$ is a submartingale, then $$\mathbb{E}[X_N \mid \mathcal{F}_n] \ge X_n.$$
\end{prop}
\begin{proof}
    This is true when $N=n+1$. Assume it's true for $N=n+k-1$, then \[
    \mathbb{E}[X_{n+k} \mid \mathcal{F}_n] = \mathbb{E}[\mathbb{E}[X_{n+k} \mid \mathcal{F}_{n+k-1}] \mid \mathcal{F}_n] \ge \mathbb{E}[X_{n+k-1} \mid \mathcal{F}_n]
    \]
    by the submartingale property and positivity of conditional expectation.
\end{proof}
\begin{prop}
    Let $(X_n)_{n\ge 0}$ be a submartingale and $(H_n)_{n\ge 1}$ nonnegative, bounded and previsible. Let $M_n = \sum_{k=1}^{n} H_k(X_k-X_{k-1})$.
    Then $(M_n)_{n\ge 0}$ is a submartingale.
\end{prop}
\begin{proof}
    Integrability follows from integrability of $(X_n)$ and boundedness of $(H_n)$. Adaptedness follows from the previsibility of $(H_n)_n$ and the adaptedness of $(X_n)_n$. Finally, \[
    \mathbb{E}[M_n-M_{n-1} \mid \mathcal{F}_{n-1}] = \mathbb{E}[H_n(X_n-X_{n-1}) \mid \mathcal{F}_{n-1}] = H_n \mathbb{E}[X_n-X_{n-1} \mid \mathcal{F}_{n-1}] \ge 0.
    \]
\end{proof}
\begin{theorem}
    Let $S$ and $T$ be stopping times such that $S\le T$ almost surely. Let $(X_n)_n$ be a submartingale and let $M_n = X_{n \wedge T} - X_{n \wedge S}$. Then $(M_n)_{n\ge 0}$ is a submartingale. 
\end{theorem}
\begin{proof}
    Note $M_n = \sum_{k=1}^{n} \mathbbm{1}_{S<k\le T}(X_k-X_{k-1})$, and the indicator is bounded and previsible.
\end{proof}
\begin{theorem}[Optional sampling theorem]
    Suppose $S \le T$ are bounded stopping times and $(X_n)_{n}$ is a submartingale. Then $\mathbb{E}[X_S]\le \mathbb{E}[X_T]$.
\end{theorem}
\begin{proof}
    Let $M_n = X_{n \wedge T} - X_{n \wedge S}$. Let $N$ be a constant such that $T\le N$ almost surely. Then $\mathbb{E}[M_n]\ge M_0=0$ because $(M_n)$ is a submartingale. Hence we're done, since $M_N = X_{T \wedge N} - X_{S \wedge N} = X_T-X_S$.
\end{proof}

\subsection{Controlled Markov processes}

\marginpar{09 Nov 2022, Lecture 15}

Review of IB Markov Chains:
\begin{defn}
    We say a process $(X_n)_{n\ge 0}$ is a \textbf{Markov process} if $$\mathbb{P}(X_n \in A \mid \mathcal{F}_{n-1}) = \mathbb{P}(X_n \in A \mid X_{n-1})$$ for any ''nice'' set $A$ and where $\mathcal{F}_n = \sigma(X_0,X_1,\ldots,X_n)$.
\end{defn}

There are two ways to think about a Markov process.
\begin{enumerate}[(i)]
    \item Our ingredients are $X_0=x$ (the initial condition) and the transition law $P(n,x,A) = \mathbb{P}(X_n \in A \mid X_{n-1}=x)$. $P(n,x,\cdot)$ is a probability measure $~\forall n,x$.
    \item It is a random dynamical system. $X_0=x$ is given and $X_n=G(n,X_{n-1}, \xi_n)$, where $G(\cdot ,\cdot ,\cdot )$ is given and $\xi_1,\xi_2,\ldots$ are independent random variables.
\end{enumerate}
These notions are independent when $\mathbb{P}(G(n,x,\xi_n) \in A) = P(n,x,A)$.

\begin{example}
    If $(X_n)_n$ is a SSRW, $X_n=\xi_1 + \ldots \xi_n$ with $(\xi_n)_n$ i.i.d. with $\mathbb{P}(\xi = \pm 1)=\frac{1}{2}$. Then $X_n = G(n,X_{n-1},\xi_n)$ where $G(n,x,\xi) = x + \xi$.
\end{example}
\textbf{Remark.} If $(X_n)_n$ is a time--homogeneous Markov process, then $G(n,\cdot ,\cdot )$ does not depend on $n$, and $(\xi_n)$ are i.i.d. $\iff P(n,\cdot ,\cdot )$ does not depend on $n$, in which case we can surpress $n$ from the notation.

\textbf{Remark.} In fact, a time--homogeneous case is general, since if $(X_n)_n$ is a Markov process, then $Y_n=(n,X_n)$ is time--homogeneous. This is because $H(n,y,\xi_n) = (n,G(n,x,\xi_n)) = (y^{(1)}, G(y^{(1)}, y^{(2)}, \xi_n))$.

\begin{defn}
    A process $(X_n)_n$ is a \textbf{controlled Markov process} with \textbf{control} $(U_n)_{n\ge 1}$ if $(U_n)_n$ is previsible and $X^U_n = G(n,X^U_{n-1}, U_n, \xi_n)$ for a given function $G$ and an independent sequence $(\xi_n)_n$.
\end{defn}

\begin{example}
    An example from finance. Let $r$ be the interest rate, $d=1$, $S_n = S_{n-1} \xi_n$, where $\xi_n$ are i.i.d. Then $(S_n)_n$ is Markov. 

    We have a wealth process $(X_n)_n$, where \[
    X_n = (1+r)X_{n-1} + \theta_n(S_n-(1+r)S_{n-1}) = (1+r)X_{n-1} + \theta_n S_{n-1}(\xi_n-(1+r)),
    \]
    where $\theta_n$ is $\mathcal{F}_{n-1}$--measurable (it is the holding of stock between times $n-1$ and $n$).
    \vspace{1mm}
    
    This is a controlled Markov process with $U_n = S_{n-1}\theta_n$ and $G(n,x,u,\xi) = (1+r)x+u(\xi-(1+r))$, which is in fact time--homogeneous. 
\end{example}

\subsection{The Bellman equation}

\subsubsection{Stochastic optimal control}

\textbf{Goal:} Given (fixed, not random) $N$, functions $f$ and $g$, and a controlled process $(X_n^U)_n$, we want to maximize \[
\mathbb{E}\left[\sum_{k=1}^{N} f(k,U_k)+g(X_N^U) \mid X_0=x \right]
\] over controls $(U_n)_{1\le n\le N}$.

We will make a blanket assumption that everything we need is integrable.

\begin{defn}
    The \textbf{value function} for this stochastic control problem is the function 
    \begin{align*}
        V(n,x) = \max \mathbb{E}\left[\sum_{k=n+1}^{N} f(k,U_k) + g(X_N^U) \mid X_n=x \right].
    \end{align*}
\end{defn}
Note that $V(N,x)=g(x)$. 

\begin{defn}
    Say our controlled Markov process evolves according to the rule $X_n=G(n,X_{n-1},U_n,\xi_n)$. Then the \textbf{Bellman equation} for this stochastic control problem is 
    \begin{align*}
        &V(N,x) = g(x) \\
        &V(n-1,x) = \max_{u} \left[f(n,u) + \mathbb{E}[V(n,G(n,x,u,\xi_n))] \right] ~\forall 1 \le n\le N.
    \end{align*} 
\end{defn}

\marginpar{11 Nov 2022, Lecture 16}

\subsubsection{Dynamic programming principle}
Setup: Given $\xi_1,\xi_2,\ldots$ independent random variables, a function $G(\cdot ,\cdot ,\cdot, \cdot)$ and an initial condition $X_0$, for any previsible process $(U_n)_{n\ge 1}$ (for the filtration generated by $(\xi_n)_n$) construct $X_0^U = X_0$ and $X_n^U = G(n,X_{n-1}^U, U_n, \xi_n)$ for $n\ge 1.$
\vspace{1mm}

Given $f(\cdot ,\cdot )$ and $g(\cdot)$, our goal is to maximize \[
\mathbb{E}[\sum_{k=1}^{N} f(k,U_k)+g(X_N^U)]
\]
over the controls $(U_n)_n$.

Let $V(\cdot ,\cdot )$ solve the Bellman equations $V(N,x)=g(x)$ and $V(n-1,x) = \max_{u} \left[f(n,u) + \mathbb{E}[V(n,G(n,x,u,\xi_n))] \right] ~\forall 1\le n\le N, ~\forall x$.

Suppose that $\forall x,n$, there exists a maximizer $u^*(n,x)$ such that $$V(n-1,x)=f(n,u^*(n,x)) + \mathbb{E}[V(n,G(n,x,u^*(n,x)), \xi_n)].$$

Let $X^*_n = \begin{cases}
    X_0 &n=0\\
    G(n,X_{n-1}^*, u^*(n,X_{n-1}^*),\xi_n) &n\ge 1.
\end{cases}$
\begin{theorem}[Dynamic programming principle]
    The control $U^*_n = U^*(n,X_{n-1}^*)$ $~\forall n\ge 1$ is optimal and $V(\cdot ,\cdot )$ is the value function.
\end{theorem}
For the proof we need the following lemma:
\begin{lemma}
    Let $X$ be $\mathcal{G}$--measurable and let $Y$ be independent of $\mathcal{G}$. Then (assuming integrability) $\mathbb{E}[h(X,Y) \mid \mathcal{G}] = \mathbb{E}[h(x,Y)]|_{x=X}.$
    \vspace{1mm}
    
    An alternative notation is to write \[
    \mathbb{E}[h(X,Y) \mid \mathcal{G}](\omega) = \mathbb{E}[h(X(\omega),Y)] \text{ for almost all }\omega.
    \]
\end{lemma}
\begin{proof}[Outline of the proof, non--examinable]
    This is clearly true if $h(x,y)=\phi(h)\psi(y)$. Now apply the monotone class theorem from II Probability \& Measure.
\end{proof}
\begin{proof}[Proof of the dynamic programming principle]
    Note that for any control $(U_n)_n$ we have 
    \begin{align*}
        &\mathbb{E}[f(n,u_n)+V(n,X_{n-1}^U,U_n,\xi_n) \mid \mathcal{F}_{n-1}] = \\
        &=(f(n,u)+\mathbb{E}[V(n,x,u,\xi_n)])|_{u=U_n,x=X_{n-1}^U} \le \\
        &\le V(n-1,x) |_{u=U_n,x=X_{n-1}^U} = V(n-1,X_{n-1}^U)
    \end{align*}
    with equality if $U_n=U^*(n,x_{n-1}^U)$.

    Fix $(U_n)_n$. Let \[
    M_n = \sum_{k=1}^{n} f(k,U_k) + V(n,X_n^U).
    \]

    The claim is that $(M_n)_n$ is a supermartingale in general and a martingale if $U_n=U^*_n$. Indeed, adaptedness is obvious and integrability we assume. Now
    \begin{align*}
        \mathbb{E}[M_n-M_{n-1} \mid \mathcal{F}_{n-1}] = \mathbb{E}[f(n,u_n) + V(n,X_n^U) \mid \mathcal{F}_{n-1}] - V(n-1,X_{n-1}^U) \le 0
    \end{align*}
    with equality if $U_n=U_n^*$. Then
    \begin{align*}
        \mathbb{E}\left[\sum_{k=1}^{N} f(k,U_k)+g(X_N) \mid \mathcal{F}_{n-1}\right] = \mathbb{E}[M_N \mid \mathcal{F}_{n}] \le M_n = \sum_{k=1}^{n} f(k,U_k)+V(n,X_n^U),
    \end{align*}
    i.e. $V(n,X_n^U) \ge \mathbb{E}[\sum_{k=n+1}^{N} f(k,U_k)+ g(X_N^n) \mid \mathcal{F}_n]$ with equality if $U_n=U_n^* ~\forall n$. Hence $$V(n,x) = \max_{(U_k)_{n+1\le k\le N}} \mathbb{E}\left[\sum_{k=n+1}^{N} f(k,U_k)+g(X_N^U) \mid X_n^U=x\right]$$
    by the tower property as $X_n$ is $\mathcal{F}_n$--measurable and $U_n^* = U^*(k,X_k^U)$ is $\sigma(X_n^U)$ measurable for $k \ge n+1$.
\end{proof}
\textbf{Remark.} This technique of proof is sometimes called the Martingale principle of optimal control (MPOC), i.e. find a function $V$ such that $(V(n,X_n^U))_n$ is a supermartingale for all $(U_n)_n$, martingale for one $(U^*_n)_n$ and $V(N,X_n^U)$ matches the objective function (or some other similar $V$).

\subsubsection*{Optimal investment}

Say we have $d=1$ risky assets, the interest rate is $r$ and $S_n=S_{n-1}\xi_n$ and $(\xi_n)_n$ are i.i.d. Our wealth then is \[
X_n= (1+r)X_{n-1}+\theta_n(S_n-(1+r)S_{n-1}) = (1+r)X_{n-1}+ \eta (\xi_n-(1+r)),
\]
where $\eta_n=\theta_n S_{n-1}$ is the total amount held in risky assets from time $n-1$ to $n$.

Our goal is to maximize $\mathbb{E}[(U(X_N))]$ for a given utility function. Write down the Bellman equations $V(N,x)=U(x)$ and $V(n-1,x) = \max_{\eta} \mathbb{E}[V(n,(1+r)x-\eta(\xi_n-(1+r)))]$. And we're done -- we have the optimal solution, we just need to solve the one--period case $n$ times (for all initial conditions). This is generally difficult, but let us suppose $U(x)=-e^{-\gamma x}$ (CARA), so $V(N,x)= -e^{-\gamma x}$. Then \[
V(N-1,x) = \max_{\eta} \mathbb{E}[\exp \left(-\gamma((1+r)x + \eta(\xi_N - (1+r)))\right)] = -e^{-\gamma(1+r)x}\alpha
\]
for $\alpha = \min_{\eta} \mathbb{E}[e^{-\gamma \eta(\xi-(1+r))}]$, where $\xi$ has the same law as $\xi_n ~\forall n$.

We guess that $V(n,x)=U((1+r)^{N-n}x)\alpha^{N-n}$, and we can check that it is actually true by induction.

So the optimal portfolio in this case is 
\begin{align*}
    \theta^*_n = \frac{\eta_n^*}{S_{n-1}} = \frac{t^*}{\gamma(1+r)^{N-n}S_{n-1}}
\end{align*}
where $t^* = \text{argmin}~\mathbb{E}[e^{-t(\xi-(1+r))}]$.
\vspace{1mm}

Another example of optimal investment, this time with consumption:

\marginpar{14 Nov 2022, Lecture 17}
\vspace{1mm}

Let us have the same setup as above: $r$ is the interest rate, $d=1$, $S_n=S_{n-1}\xi_n$ with $(\xi_n)_n$ IID. Let $C_n$ be the consumption between time $n-1$ and $n$, which is $\mathcal{F}_{n-1}$--measurable (so $(C_n)_n$ is previsible). Then our wealth is \[
X_n = (1+r)(X_{n-1}-C_n) + \theta_n(S_n-(1+r)S_{n-1}).
\]
Let $\eta_n=S_{n-1}\theta_n$, then $(C_n,\eta_n)_{n\ge 1}$ are our controls. A natural goal is to try and maximize \[
    \mathbb{E}\left[\sum_{k=1}^{N} U(C_k)+U(X_N)\right].
\]
The Bellman equations are 
\begin{align*}
    &V(N,x)=U(x)\\
    &V(n-1,x) = \max_{c,\eta} \{U(c) + \mathbb{E}[V(n,(1+r)(x-c) + \eta(\xi-(1+r)))]\}
\end{align*}
where $\xi$ is IID, so has the distribution of $\xi_n ~\forall n$.

Generally, we can now make no more progress. But if we assume that our utility function is CRRA, i.e. $U(x)=\frac{x^{1-R}}{1-R}$ for $R>0, R\neq 1$, then we can go further.
\vspace{1mm}

We guess that the solution is $V(n,x)=U(x)A_n$ (since the solution scales with $x$). This is true for $n=N$ with $A_N=1$. Assume now that $V(k,x)=A_k U(x)$ for some $k \in \mathbb{N}$. Then
\begin{align*}
    &V(k-1,x)= \max_{c,\eta} \{U(c)+\mathbb{E}[A_k U((1+r)(x-c)+\eta(\xi-(1+r)))]\} = \\
    &\max_{c,\eta} \left\{U(x)\left(\frac{c}{x}\right)^{1-R} + A_k U(x)\mathbb{E}\left[\left((1+r)(1-\frac{c}{x})+\frac{\eta}{x}(\xi-(1+r))\right)^{1-R}\right]\right\} = (\star).
\end{align*}
Let $s=\frac{c}{x}$ and $t = \frac{\eta}{x-c}$ be our new controls. Let $t^* = \text{argmax }U((1+r)+t(\xi-(1+r)))$ and let $\alpha = (1-R)\max \mathbb{E}[U((1+r)+t(\xi-(1+r)))]$. So now
\begin{align*}
    (\star) = \max_s \left\{U(x)s^{1-R} + (1-s)^{1-R}A_k \alpha U(x) \right\}.
\end{align*}
At the maximum, $s^{-R}=(1-s)^{-R}A_k \alpha$, so $s^* = \frac{1}{1+(A_k \alpha)^{1/R}}$.
\vspace{1mm}

Hence $(\star)=U(x)A_{k-1}$, where for all $k$ we have
\[
A_{k} = \left(\frac{1-\alpha^{\frac{N-k+1}{R}}}{1-\alpha^{1/R}} \right)^R.
\]
So the optimal strategy has 
\begin{align*}
    &C_n^* = X_{n-1}^* s_n^* = \frac{X^{*}_{n-1}}{1+(A_n \alpha)^{1/R}} \\
    &\theta_n^* = \frac{\eta_n^*}{S_{n-1}}- \frac{t^* (X_{n-1}^*-C_n^*)}{S_{n-1}}.
\end{align*}
\subsubsection{Infinite--horizon problems}
Consider a controlled Markov process $(X_n^U)_n$ of the form $X_n^U = G(X_{n-1}^U,U_n,\xi_n)$ where $(\xi_n)_n$ are IID. Our problem is to maximize \[
\mathbb{E}\left[\sum_{k=1}^{\infty} \beta^{k-1}f(U_k)\right]
\]
given $f$, $0<\beta<1$ and $X_0$.
\vspace{1mm}

The value function is $$\mathbb{E}\left[\sum_{k=1}^{\infty} \beta^{k-1}f(U_k) \mid  X_0 = x\right],$$
and the Bellman equation is \[
V(x) = \max_{u} \left\{f(u)+\beta \mathbb{E}[V(G(x,u,\xi))] \right\}
\]
where $\xi$ has the same distrution as $\xi_n ~\forall n$.

\begin{theorem}
    Suppose $f(u)\ge 0 ~\forall u$ and suppose that $V$ is nonnegative and solves the Bellman equation. Let $u^*(x) = \text{argmax }f(u) + \beta \mathbb{E}[V(G(x,u,\xi))]$ and let \[
    X_n^* = \begin{cases}
        X_0 &n=0\\
        G(X_{n-1}^*,u^*(X_{n-1}^*),\xi_n) &n\ge 1.
    \end{cases}
    \]
    If $\beta^n \mathbb{E}[V(X_n^*)] \to 0$ as $n \to \infty$, then $(U_n^*)^*$ is optimal.
\end{theorem}
\begin{proof}
    Given $(U_n)_n$, let \[
    M_n = \sum_{k=1}^{n} \beta^{k-1}f(u_k) + \beta^n V(X_n^U).
    \]
    Note $(M_n)_{n\ge 0}$ is a supermartingale, as \[
    \mathbb{E}[M_n-M_{n-1} \mid \mathcal{F}_{n-1}] = \beta^{n-1}\left(f(U_n)+\beta \mathbb{E}[V(X_n^U) \mid \mathcal{F}_{n-1}] - V(X_{n-1})\right)\le 0
    \] with equality if $U_n = U_n^*$. Now note that as $V\ge 0$,
    \begin{align*}
        &V(x)\ge V(x) - \beta^n \mathbb{E}[V(X_n^U)] = M_n - \beta^n \mathbb{E}[V(V_n^U)] \\
        &\ge \mathbb{E}[M_n] - \beta^n \mathbb{E}[V(X_n^U)] = \mathbb{E}[\sum_{k=1}^{n} \beta^{k-1}f(U_k)] \to \mathbb{E}[\sum_{k=1}^{\infty} \beta^k f(U_k)]
    \end{align*}
    by monotone convergence theorem (as $f\ge 0$).
    \vspace{1mm}
    
    For $U_n=U_n^*$, we have 
    \begin{align*}
        V(x)=\mathbb{E}[\sum_{k=1}^{n} \beta^{k-1}f(U_k)+\beta^n \mathbb{E}[V(X_n^*)]] \to \mathbb{E}[\sum_{k=1}^{\infty} \beta^{k-1}f(U_k^*)].
    \end{align*}
\end{proof}

\subsubsection*{Optimal stopping problems}

\marginpar{16 Nov 2022, Lecture 18}

Given a Markov process of the form $X_n = G(n,X_{n-1},\xi_n)$ for $n\ge 1$ and $(\xi_n)_{n\ge 1}$ independent, our goal is to maximize $\mathbb{E}[f(X_T)]$ over stopping times $T\le N$ (for $N$ not random). 

The Bellman equations are 
\begin{align*}
    &V(N,x)=f(x) ~\forall x\\
    &V(n-1,x) = \max \{f(x), \mathbb{E}[V(G(n,x,\xi_n))]\} ~\forall x, 1\le n\le N.
\end{align*}
\begin{theorem}
    Let $T^* = \inf \{n\ge 0 \mid V(n,X_n)=f(X_n)\}$. Then $T^*$ is optimal.
\end{theorem}
Let $\mathcal{S}$ and $\mathcal{C}$ be sets (standing for ''stop'' and ''continue'') defined as 
\begin{align*}
    &\mathcal{C}=\{(n,x) \mid V(n,x)>f(x)\}\\
    &\mathcal{S}=\{(n,x) \mid V(n,x)=f(x)\}.
\end{align*}
So our Markov process wiggles around in space (draw a diagram) until it hits $S$, and we claim we should then immediately stop. This is strongly related to the concept of American and European contingent claims -- the owner of an American claim wants to solve the optimal stopping problem to know when to cash out his claim for the most money.
\begin{proof}
    Let $M_n = V(n,x_n)$ for $n\ge 0$. Note $M_n \ge f(x_n) ~\forall n$. Also, \[
    M_{n-1} \ge \mathbb{E}[V(n,X_n) \mid \mathcal{F}_{n-1}] = \mathbb{E}[M_n \mid \mathcal{F}_n]
    \] by the Bellman equations. So $(M_n)_n$ is a supermartingale. Hence for any stopping time $T\le N$, $\mathbb{E}[M_T]\le M_0 = V(0,X_0)$ by the optional sampling theorem, hence $\mathbb{E}[f(X_T)]\le V(0,X_0)$. So we have a bound, so we just want to show we can achieve it.

    Note that $M_{n \wedge T^*}$ is a martingale, since on the set $\{n\le T^*\}$ we have $(n-1,X_{n-1}) \in \mathcal{C}$, so $V(n-1,X_{n-1})= \mathbb{E}[V(n,X_n) \mid \mathcal{F}_{n-1}]$. Hence \[
    \mathbb{E}[f(X_{T^*})] = \mathbb{E}[V(T^*,X_{T^*})] = \mathbb{E}[M_{T^*}] = M_0 = V(0,X_0).
    \]
\end{proof}
\subsection{Multiperiod arbitrage}
\textbf{Setup:} We have interest rate $r$, $d$ risky assets with prices $(S_n)_n$ (which are adapted). An investor can pick a strategy $(\theta_n)_n$ (which is previsible), with $\theta_n$ denoting the portfolio between times $n-1$ and $n$. The investor's wealth at time $n$ is $X_n$, which evolves as 
\begin{align*}
    &X_n = (1+r)X_{n-1} + \theta_n^\top(S_n-(1+r)S_{n-1})\\
    \implies &X_n = (1+r)^n X_0 + \sum_{k=1}^{n} (1+r)^{n-k}\theta_k^\top(S_k - (1+r)S_{k-1}).
\end{align*}
The investor holds $\theta^0_n = X_{n-1} - \theta_n^\top S_{n-1}$ in the risk--free asset between times $n-1$ and $n$.
\begin{defn}
    An \textbf{arbitrage} is a previsible process $(\phi_n)_{1\le n\le N}$ (for $N$ finite and not random) such that 
    \begin{align*}
        &\sum_{k=1}^{N} (1+r)^{N-k} \phi_k^\top(S_k-(1+r)S_{k-1})\ge 0 \text{ with probability 1.}\\
        &\sum_{k=1}^{N} (1+r)^{N-k} \phi_k^\top(S_k-(1+r)S_{k-1})>0 \text{ with probability}> 0.
    \end{align*}
\end{defn}
\begin{defn}
    A measure $\mathbb{Q}$ equivalent to $\mathbb{P}$ is \textbf{risk--neutral} if $$\mathbb{E}^\mathbb{Q}[S_n \mid \mathcal{F}_{n-1}] = (1+r)S_{n-1} ~\forall n\ge 1.$$
\end{defn}
\begin{theorem}[Fundamental theorem of asset pricing]
    In a finite time horizon market model, there is no arbitrage if and only if there exists a risk--neutral measure.
\end{theorem}
\subsection{The binomial model}
We look at the Cox-Ross-Rubenstein binomial model.

\textbf{Setup:} $r$ is the interest rate, we have $d=1$ risky asset with prices $S_n=S_{n-1}\xi_n$ for $n\ge 1$, where $S_0>0$ and the $(\xi_n)_n$ are IID with $\mathbb{P}(\xi_n=1+b)=p=1- \mathbb{P}(\xi_n = 1+a)$ where $-1<a<b$. We can now draw a tree of probabilities and stock prices.

\begin{theorem}
    There exists a risk--neutral measure on a finite horizon if and only if $a<r<b$. If the risk--neutral measure exists, then it is unique.
\end{theorem}
\begin{cor}
    There is no arbitrage if and only if $a<r<b$ (by FTAP).
\end{cor}
\begin{proof}
    Suppose a risk--neutral measure $\mathbb{Q}$ exists. Then 
    \begin{align*}
        &\mathbb{E}^{\mathbb{Q}}[S_n \mid \mathcal{F}_{n-1}] = (1+r)S_{n-1} = \\
        &(1+b)S_{n-1}\mathbb{Q}(\xi_n=1+b \mid \mathcal{F}_{n-1}) + (1+a)S_{n-1}\mathbb{Q}(\xi_n=1+a \mid \mathcal{F}_{n-1}) \\
        &\implies \mathbb{Q}(\xi_n = 1+b \mid \mathcal{F}_{n-1}) = \frac{r-a}{b-a}=q.
    \end{align*}
    Note $q \in (0,1) \iff a<r<b$ and there's only one solution.
\end{proof}
\begin{cor}[Consequence of the proof]
    When $a<r<b$, then under the unique risk--neutral measure $\mathbb{Q}$, the $(\xi_n)_n$ are IID with $\mathbb{Q}(\xi_n=1+b)=q = 1 - \mathbb{Q}(\xi_n=1+a) ~\forall n$.
\end{cor}
\begin{proof}
    Note that $\mathbb{Q}(\xi_n = 1+b \mid \mathcal{F}_{n-1})=q$ is independent of $\mathcal{F}_{n-1}$, so $\xi_n$ is independent of $\mathcal{F}_{n-1}$.
\end{proof}

\marginpar{18 Nov 2022, Lecture 19}

\subsubsection*{Pricing and hedging European claims in the binomial model}
\begin{defn}
    A \textbf{European contingent claim} is an asset whose payout at some fixed time $N$ (the maturity date or the expiry date) is specified by a $\mathcal{F}_N$--measurable random variable $Y$.
\end{defn}
\begin{defn}
    A claim is called \textbf{vanilla} if its payout is of the form $Y=g(S_N)$ (where $(S_n)_n$ are the prices of some given assets).
    \vspace{1mm}
    
    A claim that is not vanilla (the payout depends on the whole path $(S_n)_n$) is called \textbf{exotic}. 
\end{defn}
\begin{example}
    A European call option, where $g(x)=(x-K)^+$, is vanilla.
\end{example}
From now on, consider the binomial model with $r$ the interest rate, $d=1$, $S_n = S_{n-1}\xi_{n}$ for $(\xi_n)_n$ IID, $\mathbb{Q}(\xi_n=1+b)=q=\frac{r-a}{b-a}=1-\mathbb{Q}(\xi_n=1+a)$, where $-1<a<r<b$ are given.
\vspace{1mm}

\textbf{Motivation.} We want to price and hedge (i.e. find a replicating portfolio) a vanilla European claim with payout $Y=g(S_N)$.

To replicate, we solve \[
    \theta_N^0(1+r) + \theta_N S_N = g(S_N),
\]
where $\theta_N^0, \theta_N$, which are the amount in the bank and the number of shares respectively, are $\mathcal{F}_N$--measurable. So we solve 
\begin{align*}
    \begin{cases}
        &\theta_N^0(1+r) + \theta_N S_{N-1}(1+b) = g(S_{N-1}(1+b))\\
        &\theta_N^0(1+r) + \theta_N S_{N-1}(1+a) = g(S_{N-1}(1+a))
    \end{cases}\\
    \implies \theta_N = \frac{g(S_{N-1}(1+b))-g(S_{n-1}(1+a))}{S_{N-1}(b-a)}.
\end{align*}
Similarly $\theta_N^0$, the cost of replication, solves 
\begin{align*}
    &\theta_N^0 + \theta_N S_{N-1} = \frac{1}{1+r} \left(q g(S_{n-1}(1+b)) + (1-q)g(S_{N-1}(1+a))\right) =\\
    &= \frac{1}{1+r} \mathbb{E}^\mathbb{Q}[g(S_N) \mid \mathcal{F}_{N-1}] = \pi_{N-1},     
\end{align*}
the time $N-1$ no arbitrage price.
To get the time $N-2$ price, we solve the same process for the time $N-1$ no arbitrage price, etc.

\begin{theorem}
    Let 
    \begin{align*}
        &V(n,s) = \frac{\mathbb{E}^\mathbb{Q}[g(S_N) \mid S_{n}=s]}{(1+r)^{N-n}}, ~0\le n\le N \\
        & \theta_n = \frac{V(n,S_{N-1}(1+b))-V(n,S_{n-1}(1+a))}{S_{N-1}(b-a)}.
    \end{align*}
    Let 
    \begin{align*}
        &X_0 = V(0,S_0)\\
        &X_n = (1+r)X_{n-1} + \theta_n(S_n-(1+r)S_{n-1}) \text{ for }1\le n\le N.
    \end{align*}
    Then $X_n = V(n,S_n)$ for all $0\le n\le N$ and in particular $X_N=g(S_N)$ (by Markov chains). So $(\theta_n)_n$ is a replicating trading strategy for the claim.
\end{theorem}
\begin{proof}
    Note that
    \begin{align*}
        (1+r)V(n-1,s) = qV(n,S(1+b)) + (1-q)V(n,S(1+a))
    \end{align*}
    since $(S_n)_n$ is a Markov chain. Now suppose $X_k=V(k,S_k)$ for $k\le n-1$, so \[
    X_n=(1+r)V(n-1,S_{n-1}) + \theta_n(S_n-(1+r)S_{n-1}).
    \]
    Plug in the expression for $\theta_n$ given in the statement of the theorem to get
    \begin{align*}
        &X_n = qV(n,S_{n-1}(1+b)) + (1-q)V(n,S_{n-1}(1+a))+\\
        &\frac{V(n,S_{n-1}(1+b))-V(n,S_{n-1}(1+a))}{b-a}(\xi_n-(1+r)) = \\
        &= V(n,S_{n-1}(1+b))\left(\frac{\xi_n-(1+a)}{b-a}\right) + V(n,S_{n-1}(1+a))\left(\frac{1+b-\xi_n}{b-a}\right) = \\
        &=V(n,S_n),
    \end{align*}
    since $\frac{\xi_n- (1+b)}{b-a} =\mathbbm{1}(\xi_n=1+a)=1-\mathbbm{1}(\xi_n=1+b)$. Hence we're done by induction.
\end{proof}
\textbf{Remark.} $M_n = (1+r)^{-n} V(n,S_n)$, the discounted prices of the claim, is a $\mathbb{Q}$--martingale (as it should be by FTAP). Note $V(n,S_n) = \mathbb{E}^\mathbb{Q}[g(S_N) \mid S_N] = \mathbb{E}^\mathbb{Q}[g(S_N) \mid \mathcal{F}_n]$, since $(S_n)_n$ is Markov.
\vspace{1mm}

Recall that $\mathbb{Q}$ is risk--neutral $\iff$ $\mathbb{E}[S_n \mid \mathcal{F}_{n-1}]=(1+r)S_{n-1} ~\forall n\ge 1 \iff \left(\frac{S_n}{(1+r)^n}\right)_{n\ge 0}$ is a $\mathbb{Q}$--martingale. So risk--neutral measures are sometimes called equivalent martingale measures (with respect to the bank account).

\subsubsection*{American contingent claims}
\begin{defn}
    An \textbf{American claim} is specified by the adapted process $(Y_n)_{0\le n\le N}$. The owner of the claim chooses a stopping time $T\le N$ and receives the payout $Y_T$ at time $T$.
\end{defn}
\begin{example}
    An American call option with payout $Y_n = (S_n-K)^+$.
\end{example}
\begin{theorem}
    The no--arbitrage price of a vanilla American claim with payout $Y_n=G(S_n)$ for $0<-n\le N$ in the binomial model is $\pi_n = V(n,S_n)$ where $V(n,s)=g(s) ~\forall s$ and 
    \begin{align*}
        V(n-1,s) = \max \{g(s), \underbrace{\frac{1}{1+r}\left(qV(n,s(1+b)) + (1-q)V(n,s(1+a))\right)}_{=\frac{1}{1+r}\mathbb{E}^\mathbb{Q}[V(n,S_n)\mid S_{n-1}=s]}\}
    \end{align*}
    and an optimal exercise time is \[
    T^* = \min \{n\ge 0 \mid V(n,S_n)=g(S_n)\}
    \]
\end{theorem}

\marginpar{21 Nov 2022, Lecture 20}

In fact, a slightly more general theorem is true:

\begin{theorem}
    Suppose $(Y_n)_n$ specifies the payout of an American claim in a complete market. Then the price at time $n$ is 
    \begin{align*}
        &\pi_N=Y_N \\
        &\pi_{n-1}=\max \{Y_{n-1},\frac{1}{1+r}\mathbb{E}^\mathbb{Q}[\pi_n \mid \mathcal{F}_{n-1}]\},
    \end{align*}
    where $\mathbb{Q}$ is a risk--neutral measure. Then an optimal exercise time is $T^* = \min \{n\ge 0 \mid \pi_n = Y_n\}$.
\end{theorem}
A consequence of this:
\begin{theorem}
    If $\left(\frac{Y_n}{(1+r)^n}\right)_{n\ge 0}$ is a submartingale under $\mathbb{Q}$, then an optimal exercise time is $T^*=N$. That is, the American claim is essentially a European claim with time $N$ payout $Y_N$.
\end{theorem}
\begin{proof}
    The claim is that $\pi_n = \frac{\mathbb{E}[Y_N \mid \mathcal{F}_n]}{(1+r)^{N-n}}$ for all $0\le n \le N$. This is true for $n=N$, so we induct.
    \vspace{1mm}
    
    Suppose it is true for $n=k$, then 
    \begin{align*}
        &\pi_{k-1} = \max \{Y_{k-1}, \frac{\mathbb{E}[\pi_k \mid \mathcal{F}_{k-1}]}{1+r}\} = \\
        &= \max \{Y_{k-1}, \frac{1}{1+r}\mathbb{E}[\frac{\mathbb{E}[Y_{N} \mid \mathcal{F}_k]}{(1+r)^{N-k}}\mid \mathcal{F}_{k-1}]\} = \\
        &= \max \{Y_{k-1}, \frac{\mathbb{E}[Y_N \mid \mathcal{F}_{k-1}]}{(1+r)^{N-k+1}}\} = \\
        &= \frac{\mathbb{E}[Y_N \mid \mathcal{F}_{n-1}]}{(1+r)^{N-k+1}}
    \end{align*}
    since $\left(\frac{Y_n}{(1+r)^{n}}\right)_n$ is a submartingale, so we're done by induction.
\end{proof}
\begin{theorem}
    Let $d=1$ and $(S_n)_n$ be the prices of the asset. Then $\frac{(S_n-k)^+}{(1+r)^n}$ is a submartingale under $\mathbb{Q}$.
\end{theorem}
\begin{proof}
    Left as exercise. Hint: use conditional version of Jensen's inequality, the tower property, and the martingale property of $\left(\frac{S_n}{(1+r)^n}\right)_n$.
\end{proof}

We deduce that it is optimal to not exercise an American call early (i.e. we treat it as a European call).
\vspace{1mm}

Aside: So why doesn't this happen in real markets? It's because of some of the assumptions we made. A big one is that we assumed that there were no dividends - but actual stock does pay dividends. Another is that owning stock gives you a right to vote, so a big investor might exercise a lot of calls early to be able to steer the vote in a direction favorable for them. End of aside.

\begin{defn}
    A \textbf{put option} is the right, but not the obligation, to sell a given asset at a fixed price $K$ (called the strike price). The payout at time $n$ is $(K-S_n)^+$.
\end{defn}
Just like calls, put options can be both American and European.
\vspace{1mm}

\textbf{Remark.} Unlike calls, American puts usually have nontrivial optimal exercise times.
\begin{theorem}
    Consider a market with a stock, bank account, and a call option on the stock with maturity $N$ and strike $K$. Then a European put with maturity $N$ and strike $K$ can be replicated (i.e. attained).
\end{theorem}
\begin{proof}
    Note that $(K-S_N)^+ = K-S_N + (S_N-K)^+$ (as $a^+ - (-a)^+ =a ~\forall a \in \mathbb{R}$), so hold $\theta_n^0 = \frac{K}{(1+r)^{N-n+1}}$ in the bank between $n-1$ and $n$, sell 1 stock and buy 1 call to replicate the payout of the put.
\end{proof}
\vspace{1mm}

\textbf{Put-call parity formula}

We have 
\begin{align*}
    P_n = \frac{K}{(1+r)^{N-n}} - S_n + C_n,
\end{align*}
where $P_n$ and $C_n$ are the no--arbitrage prices of the put and call.


\section{Continuous--time finance}
\textbf{Motivation:} In the binomial model, $S_n=S_0\xi_1\xi_2\ldots \xi_n$ where $(\xi_n)_n$ are IID and take values $1+a$ or $1+b$. Then \[
\log S_n = \log S_0 + X_1 + \ldots + X_n,
\]
where $X_i = \log \xi_i$. This is a random walk. Let $\hat{S}_t = S_{t/\delta}$ for $t=n \delta$, $n \in \mathbb{Z}_{\ge 0}$, $\delta>0$ small. Hence \[
\log \hat{S_t} = \log S_0 + \mu t + \sigma W_t, 
\] 
where $\mu = \frac{\mathbb{E}[X]}{\delta}$, $\sigma^2 = \frac{\text{Var}(X)}{\delta}$. This defines $W_t$, which has nice proprties. Note that $W_t-W_s$ for $0\le s<t$
is independent of $W_n$ for $0\le u\le s$ (where $\frac{u}{\delta},\frac{s}{\delta},\frac{t}{\delta}$ are all integers). Note that \[
W_t-W_s \approx N(0,t-s)
\]
when $\delta$ is very small (by the central limit theorem). End of motivation.
\vspace{1mm}

\begin{defn}
    A \textbf{Brownian motion} $(W_t)_{t\ge 0}$ is a process with the following properties:
    \begin{itemize}
        \item It is a continuous process ($t \mapsto W_t$ is continuous)
        \item $W_0=0$.
        \item $W_t - W_s$ is independent of $\mathcal{F}_s$, where $\mathcal{F}_s=\sigma(W_u,  0\le u\le s)$
        \item $W_t-W_s \sim N(0,t-s) ~\forall 0\le s\le t$.
    \end{itemize}
\end{defn}

*example picture of a path of Brownian motion*. This is erratic everywhere, and continues to be erratic when we zoom in, so the derivative of $t \mapsto W_t$ does not exist anywhere.

\marginpar{25 Nov 2022, Lecture 21}

\subsection{Properties of Brownian motion}

\begin{theorem}[Wiener, 1923]
    Brownian motion exists.
\end{theorem}
\textbf{Remark.} In America, Brownian motion is called a Wiener process.

\begin{theorem}
    Brownian motion is a Markov process and a martingale (in its own filtration).
\end{theorem}
\begin{proof}
    Martingale: $W_t$ is integrable since normal, and by independence and $W_t-W_s \sim N(0,t-s)$ we have $\mathbb{E}[W_t - W_s \mid \mathcal{F}_s] = \mathbb{E}[W_t-W_s]=0$.
    \vspace{1mm}
    
    Markov: 
    \begin{align*}
        &\mathbb{E}[f(W_t) \mid \mathcal{F}_s] = \mathbb{E}[f(W_t-W_s+W_s) \mid \mathcal{F}_s] = \\
        &=\mathbb{E}[f(W_t-W_s+x)]\mid_{x =W_s} = \mathbb{E}[f(W_t-W_s+W_s)\mid W_s] = \mathbb{E}[f(W_t) \mid W_s].
    \end{align*}
\end{proof}
\begin{defn}
    A Gaussian process $(X_t)_{t\ge 0}$ is such that for any $0\le t_0 < t_1 <\ldots < t_n$, the random variables $X_{t_0},X_{t_1},\ldots,X_{t_n}$ are jointly normal (Gaussian), i.e. $\sum_{i=1}^{n} a_i X_{t_i}$ is normal for any constants $a_0,\ldots,a_n$.
\end{defn}
\begin{theorem}
    The following are equivalent:
    \begin{enumerate}[(i)]
        \item $(W_t)_{t\ge 0}$ is a Brownian motion.
        \item $(W_t)_{t\ge 0}$ is a continuous Gaussian process such that $\mathbb{E}[W_t]=0 ~\forall t$ and $\mathbb{E}[W_s W_t]=s \wedge t = \min(s,t)$ for all $s,t$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    (i) $\implies$ (ii): Continuity follows from the definition of being Gaussian: $$\sum_{i=1}^{n} a_i W_{t_i} = \sum_{i=1}^{n} b_i(W_{t_i}-W_{t_{i-1}}) + b_0(W_{t_0}-0),$$
    i.e. for any constants $(a_i)$ there exist constants $(b_i)$ such that this statement is true. We know $W_{t_i}-W_{T_{i-1}}$ are independent normals, and a linear combination of independent normals is normal. \vspace{1mm}
    
    We also have $\mathbb{E}[W_t]=0$ since $W_t - W_0 \sim N(0,t)$ and $W_0=0$.
    \vspace{1mm}
    
    Finally, if $s<t$, then 
    \begin{align*}
        \mathbb{E}[W_sW_t]=\mathbb{E}[W_s ^2]+\mathbb{E}[W_s(W_t-W_s)]=\text{Var}(W_s) + \mathbb{E}[W_s]\mathbb{E}[W_s-W_t]=s.
    \end{align*}
    (ii) $\implies$ (i): We need to check that $W_t-W_s$ is independent of $W_u$ for $u\le s<t$. But note that 
    \begin{align*}
        \mathbb{E}[(W_t-W_s)W_u] = \mathbb{E}[W_tW_u] - \mathbb{E}[W_sW_u]= u-u=0,
    \end{align*}
    so $W_t-W_s$ and $W_u$ are uncorrelated and hence independent (as they're Gaussian).
\end{proof}
\begin{theorem}
    Let $(W_t)_{t\ge 0}$ be a Brownian motion. Then the following are also Brownian motions:
    \begin{enumerate}[(i)]
        \item $(W_{t+T}-W_T)_{t\ge 0}$ for some constant $T\ge 0$.
        \item $(c W_{t/c^2})_{t\ge 0}$ for some $c \neq 0$.
        \item $(tW_{1/t})_{t\ge 0}$.
    \end{enumerate}
\end{theorem}
\begin{proof}[Proof of (iii)]
    The Brownian law of large numbers says that $\frac{W_t}{t} \to 0$ almost surely as $t \to \infty$. Assuming this, our motion is Gaussian, clearly continuous, we have $\mathbb{E}[t W_{1/t}]=0$ since $W_{1/t} \sim N(0,1/t)$ and \[
    \mathbb{E}[sW_{1/s}t W_{1/t}] = st \mathbb{E}[W_{1/s}W_{1/t}] = st \min\left(\frac{1}{s},\frac{1}{t}\right) = \min(s,t).
    \]
\end{proof}
\subsubsection{Brownian reflection principle}
\begin{theorem}
    Let $T_a = \inf \{t>0 \mid W_t=a\}$ (and we let $\inf \emptyset = \infty$). Then $T_a< \infty$ almost surely for all $a$.
\end{theorem}
\begin{proof}
    WLOG assume $a>0$. We need to show $\mathbb{P}(\sup_{t\ge 0} \{W_t>a\})=1 ~\forall a$. Let $Z = \sup_{t\ge 0}W_t$. Note $Z\ge W_0 = 0$. Now
    \begin{align*}
        \mathbb{P}(a < Z < b) = \mathbb{P}(a < \sup_t W_t < b) = \mathbb{P}(a < \sup_t c W_{t/c^2} <b) = \mathbb{P}\left(\frac{a}{c} < \sup_t W_t < \frac{b}{c}\right) = 0
    \end{align*}
    by sending $c \to \infty$. Hence $Z \in \{0,\infty\}$ almost surely. We want to show it is $\infty$. Let $\hat{Z} = \sup_{t\ge 1}(W_t-W_1)$. This has the same distribution as $Z$, so $\hat{Z}=0 \iff Z=0$. Let $p= \mathbb{P}(Z=0,\hat{Z}=0)=\mathbb{P}(\hat{Z}=0)<\mathbb{P}(W_1\le 0, \hat{Z}=0) = \frac{1}{2}p$ by independence, so $p=0$ and we're done. 
\end{proof}
\begin{theorem}
    $(W_{T+t}-W_T)_{t\ge 0}$ is a Brownian motion independent of $(W_t)_{t\le T}$ for any finite stopping time $T$.
\end{theorem}
\begin{proof}
    Omitted, but uses the strong Markov property of Brownian motion.
\end{proof}
\begin{theorem}[Reflection principle]
    Let $(W_t)_{t\ge 0}$ be a Brownian motion. Let $\hat{W_t} = \begin{cases}
        W_t &t\le T_a\\
        2a-W_t &t>T_a.
    \end{cases}$
    Then $(\hat{W_t})_{t\ge 0}$ is a Brownian motion. We have the key formula:
    \[
    \mathbb{P}(\max_{0\le s\le t} W_s\ge a, W_t\le b) = \mathbb{P}(W_t\ge 2a-b).
    \]
\end{theorem}
\begin{proof}
    Let $M_t = \max_{0\le s\le t}W_s$.
    Clearly $(\hat{W_t})_{t\ge 0}$ is a Brownian motion. Now note that $\{\hat{W_t} \ge 2a-b\} = \{T_a \le t\} \cap \{W_t \le b\} = $. Hence
    \begin{align*}
        \mathbb{P}(W_t \ge 2a-b) = \mathbb{P}(\hat{W_t} \ge 2a-b) = \mathbb{P}(M_t \ge a, W_t\le b).
    \end{align*}
\end{proof}
\marginpar{25 Nov 2022, Lecture 22}

Some consequences of this:
\begin{itemize}
    \item The joint distribution function satisfies
    \begin{align*}
        &F_{M_t,W_t}(a,b)=\mathbb{P}(M_t\le a,W_t\le b) = \mathbb{P}(W_t\le b) - \mathbb{P}(M_t\ge a, W_t\le b) = \\
        &= F_{W_t}(b) + F_{W_t}(2a-b) - 1 = \Phi \left(\frac{b}{\sqrt{t}}\right) + \Phi \left(\frac{2a-b}{\sqrt{t}}\right)-1
    \end{align*}
    for $0\le a\le b$, where $\Phi(x) = \mathbb{P}(Z\le x)$ where $Z\sim N(0,1)$.
    \item The joint density is 
    \begin{align*}
        f_{M_t,W_t}(a,b) = -2 f'_{W_t}(2a-b) = \frac{2(2a-b)}{t^{3/2}} \Psi\left(\frac{2a-b}{\sqrt{t}}\right),
    \end{align*}
    where $\Psi(x) = \frac{e^{-x^2/2}}{\sqrt{2\pi}}$. Note that $f_{W_t}(x)$ is defined as $f_{W_t}(x) = \frac{\Psi(\frac{x}{\sqrt{t}})}{\sqrt{t}}$, so $\Psi'(x)=-x\Psi(x)$. The LHS of the first formula is usually more useful for example sheet questions than the RHS.
\end{itemize}
\begin{example}
    \begin{align*}
        &\mathbb{E}[g(W_t)\mathbbm{1}_{\{M_t \ge a\}}] = \mathbb{E}[g(W_t) \mathbbm{1}_{\{M_t\ge a, W_t \ge a\}}] + \mathbb{E}[g(W_t)\mathbbm{1}_{\{M_t\ge a,W_t\le a\}}] = \\
        &= \mathbb{E}[g(W_t) \mathbbm{1}\{W_t\ge a\}] + \mathbb{E}[g(2a-\tilde{W}_t) \mathbbm{1}_{\{\tilde{W}_t \ge a\}})]
    \end{align*}
    where $\tilde{W}_t = \begin{cases}
        W_t &t \le T_a. \\
        2a-W_t &t \ge T_a. 
    \end{cases}$
    So the above is 
    \begin{align*}
        \mathbb{E}[g(W_t) \mathbbm{1}\{W_t\ge a\}] + \mathbb{E}[g(2a-W_t) \mathbbm{1}_{\{W_t \ge a\}})] = \int_{0}^{\infty} (g(x)+g(2a-x)) \frac{\phi(\frac{x}{\sqrt{t}})}{\sqrt{t}} \mathrm{d}x.
    \end{align*}
\end{example}

Another consequence:
\begin{align*}
    &F_{M_t,W_t}(a,b) = F_{W_t}(b)+F_{W_t}(2a-b)-1,\text{ so}\\
    &F_{M_t}(a) = \mathbb{P}(M_t\le a) = \mathbb{P}(M_t\le a, W_t\le a) = 2 F_{W_t}(a)-1 = \mathbb{P}(F_{|W_t|}\le a).
\end{align*}
Note $M_t \sim |W_t|$.
\vspace{1mm}

Furthermore,
\begin{align*}
    \mathbb{P}(T_a\le t) = \mathbb{P}(M_t \ge a) = 2(1-F_{W_t}(a)) = 2F_{W_t}(-a)
\end{align*}
by the symmetry of the normal distribution.
\vspace{1mm}

So $T_a$ has distribution $f_{T_a}(t) = \frac{a}{t^{3/2}}\phi(\frac{a}{\sqrt{t}})$.

\subsection{Cameron--Martin theorem}

Motivation: On ES1 we showed that if $Z \sim N(0,1)$, then \[
\mathbb{E}[e^{aZ-\frac{a^2}{2}}g(Z)] = \mathbb{E}[g(Z+a)].
\]
The proof was just change of variables inside an integral.
\vspace{1mm}

A generalization: Suppose $Z \sim N_n(0,I_n)$ is an $n$--dimensional standard normal. Then \[
\mathbb{E}[e^{a^\top Z-\frac{|a|^2}{2}}g(Z)] = \mathbb{E}[g(Z+a)].
\]
The proof is exactly the same, coordinate by coordinate.

\begin{theorem}[Cameron-Martin theorem]
    Let $(W_t)_{t\ge 0}$ be a Brownian motion. Fix $T>0$ (not random) and $c \in \mathbb{R}$. Then, for suitable functions $g : C[0,T] \to \mathbb{R}$,
    \[
    \mathbb{E}[e^{cW_t - \frac{c^2}{2}T}g((W_t)_{0\le t\le T})] = \mathbb{E}[g((W_t+ct)_{0\le t\le T})].
    \]
\end{theorem}
\begin{proof}[Proof (sketch).]
    By measure theory, it is enough to consider $g$ of the form $g(w)=G(w(t_1),\ldots,w(t_n))$ for $0\le t_1 \le \ldots \le t_n \le T$. We can write $G$ as \[
    G(\sqrt{t_1}z_1,\ldots,\sum_{i=1}^{n} \sqrt{t_i-t_{i-1}}z_i) = H\left(\frac{W(t_1)}{\sqrt{t_1}}, \frac{W(t_2)-W(t_1)}{\sqrt{t_2-t_1}},\ldots,\frac{W(t_n)-W(t_{n-1})}{\sqrt{t_n-t_{n-1}}}\right)
    \] where $H=H(z_1,\ldots,z_n)$. Then 
    \begin{align*}
        &\mathbb{E}\left[H\left(\frac{W_{t_1}+ct_1}{\sqrt{t_1}}, \frac{W_{t_2}-W_{t_1}+c(t_2-t_1)}{\sqrt{t_2-t_1}},\ldots,\frac{W_{t_n}-W_{t_{n-1}}+c(t_n-t_{n-1})}{\sqrt{t_n-t_{n-1}}}\right)\right] =\\
        &\mathbb{E}[g(W_t+ct)_{0\le t\le T}] = \mathbb{E}[H(Z_1+c\sqrt{t_1},Z_2+c\sqrt{t_2-t_1},\ldots,Z_n+c\sqrt{t_n-t_{n-1}})]
    \end{align*}
    where $Z_i = \frac{W_{t_i}-W_{t_{i-1}}}{\sqrt{t_i-t_{i-1}}} \sim N(0,1)$ independent. So the above is 
    \begin{align*}
        &= \mathbb{E}[H(Z+a)],
    \end{align*}
    where $a=(c\sqrt{t_i-t_{i-1}})_i$, so 
    \begin{align*}
        =\mathbb{E}[e^{a^\top Z - \frac{|a|^2}{2}}H(Z)].
    \end{align*}
    Finally note \[
    a^\top Z = \sum_{}^{} a_i Z_i = \sum_{}^{} c \sqrt{t_i-t_{i-1}} \frac{W_{t_i}-W_{t_{i-1}}}{\sqrt{t_i-t_{i-1}}} = cW_T,
    \]
    so 
    \begin{align*}
        &|a|^2 = \sum_{}^{} c^2(t_i - t_{i-1}) = c^2 T = \mathbb{E}\left[e^{cW_t-\frac{c^2}{2}T}H\left(\frac{W_{t_1}}{\sqrt{t_1}},\ldots,\frac{W_{t_n}-W_{t_{n-1}}}{\sqrt{t_n-t_{n-1}}}\right)\right] =\\ 
        &\mathbb{E}[e^{cW_t - \frac{c^2}{2}T}g((W_t)_{0\le t\le T})].
    \end{align*}
\end{proof}

\marginpar{28 Nov 2022, Lecture 23}

\subsubsection{An application of Cameron--Martin}
\begin{prop}
    Let $(W_t)_{t\ge 0}$ be a Brownian motion, $c \in \mathbb{R}$, $a\ge 0$. Then 
    \begin{align*}
        \mathbb{P}(\max_{0\le t\le T}(W_t+ct)\le a) = \mathbb{P}(W_T \le a - cT) - e^{2ca}\mathbb{P}(W_T \ge a +cT).
    \end{align*}
\end{prop} 
\begin{proof}
    By Cameron-Martin (twice) and the reflection principle from last time,
    \begin{align*}
        &\mathbb{E}[\mathbbm{1}_{\{\max_{0\le t\le T}(W_t+ct) \le a\}}] = \mathbb{E}[e^{cW_T-c^2T/2}\mathbbm{1}_{\{\max_{0\le t\le T} W_t\le a\}}] \\
        =& \mathbb{E}[e^{cW_t - c^2T/2}(1-\mathbbm{1}_{\{\max \ge a\}})] \\
        =& \mathbb{E}[e^{cW_T-c^2T/2}]-\mathbb{E}[e^{cW_T-c^2T/2}\mathbbm{1}_{W_T\ge a}] - \mathbb{E}[e^{c(2a-W_T)-c^2T/2}\mathbbm{1}_{W_T\ge a}] \\
        =&\mathbb{E}[e^{cW_T - c^2T/2}\mathbbm{1}_{\{W_{t}\le a\}}] - e^{2ac} \mathbb{E}[e^{-cW_T-c^2T/2}\mathbbm{1}_{\{W_T\ge a\}}] \\
        =& \mathbb{E}[\mathbbm{1}_{\{W_T+cT\le a\}}] - e^{2ac} \mathbb{E}[\mathbbm{1}_{\{W_T-cT \ge a\}}].
    \end{align*}
    The idea here is ''use C--M to get rid of the drift, apply reflection principle, use C--M to put the drift back''.
    \vspace{1mm}
    
    Alternatively, we could use pdf's everywhere to get that the above probability is \[
    \Phi \left(\frac{a-ct}{\sqrt{t}}\right) - e^{2ca} \Phi \left(\frac{-a-ct}{\sqrt{t}}\right),
    \]
    where $\Phi(x) = \int_{0}^{x} \frac{e^{-s^2/2}}{\sqrt{2\pi}} \mathrm{d}s$.
\end{proof}

\subsubsection{Alternative formulation of Cameron-Martin.} 

Let $(W_t)_{0\le t\le T}$ be a Brownian motion under $\mathbb{P}$. Fix $c \in \mathbb{R}$ and let $$\frac{d \mathbb{Q}}{d \mathbb{P}} = e^{cW_T-c^2T/2}$$ and $\hat{W}_t = W_t - ct ~\forall 0\le t\le T$. Then $(\hat{W}_t)_{0\le t\le T}$ is a Brownian motion under $\mathbb{Q}$.
\begin{proof}
    Fix a suitable functional $g : C[0,T] \to \mathbb{R}$. Then 
    \begin{align*}
        \mathbb{E}^\mathbb{Q}[g((\hat{W}_t)_{0\le t\le T})] = \mathbb{E}^{\mathbb{P}}[e^{cW_T-cT^2/2}g((W_t-ct)_{0\le t\le T})] = \mathbb{E}^{\mathbb{P}}[g((W_t)_{0\le t\le T})]
    \end{align*}
    by the first formulation of Cameron--Martin. Since $g$ is arbitrary, the probability law of $(W_t)_{0\le t\le T}$ under $\mathbb{P}$ is the same as that of $(\hat{W}_t)_{0\le t\le T}$ under $\mathbb{Q}$ and we're done.
\end{proof}

\subsection{The heat equation}
\begin{prop}
    Let $g$ be suitable and set $$U(t,x) = \mathbb{E}[g(x+\sqrt{t}Z)]$$ where $Z \sim N(0,1)$. Then $u$ solves \[
    \frac{\partial u}{\partial t} = \frac{1}{2} \frac{\partial^2 u}{\partial x^2}
    \] with initial condition $u(0,x)=g(x) ~\forall x$.
\end{prop}
\begin{proof}[Sketch of proof.]
    If $g$ is smooth enough, then using ES1 and integration by parts,
    \begin{align*}
        \frac{\partial u}{\partial t} = \mathbb{E}\left[g'(x+\sqrt{t}Z)\frac{Z}{2\sqrt{t}}\right] = \frac{1}{2}\mathbb{E}[g''(x+\sqrt{t}Z)] = \frac{1}{2} \frac{\partial^2 u}{\partial x^2}.
    \end{align*}
\end{proof}
\textbf{Remark.} If $g$ is not smooth, then use the formula \[
u(t,x) = \int \frac{e^{-\frac{(y-x)^2}{2t}}}{\sqrt{2\pi t}}g(y) \mathrm{d}y.
\]
Then $x+\sqrt{t}Z \sim N(x,t)$, which is exactly the density in the above integral.

\begin{prop}
    Let $(W_t)_{0\le t\le T}$ be a Brownian motion and \[
    \frac{\partial V}{\partial t} + \frac{1}{2}\frac{\partial^2 V}{\partial x^2} = 0.
    \]
    Then $(V(t,W_t))_{t\ge 0}$ is a martingale (again assuming suitability).
\end{prop}
\begin{proof}
    Fix $T\ge 0$. Let $u(t,x) = V(T-t,x)$ and note that $\frac{\partial u}{\partial t} = \frac{1}{2} \frac{\partial^2 u}{\partial x^2}$. Now, since $W_T-W_t \sim N(0, T-t)$ is independent of $\mathcal{F}_t$ and $W_t$ is $\mathcal{F}_t$--measurable,
    \begin{align*}
        &\mathbb{E}[V(T,W_T) \mid \mathcal{F}_t)] \\ =& \mathbb{E}[V(T,x+\sqrt{T-t}Z)] \mid_{X=W_t} \\=& \mathbb{E}[u(0,x+\sqrt{T-t}Z)] \mid_{X=W_t} \\
        =& u(T-t,x) \mid_{X=W_t} \\ =& V(t,W_t).
    \end{align*}
\end{proof}
\subsection{The Black-Scholes model}
We have interest rate $r$, $d=1$ (so one stock) with stock price at time $t$ given by \[
S_t = S_0 e^{\mu t + \sigma W_t},
\]
where $(W_t)_{t\ge 0}$ is a Brownian motion and $\sigma>0$.

\begin{theorem}
    Fix $T > 0$. Let $c = \frac{r-u}{\sigma} - \frac{\sigma}{2}$ and let $\frac{d \mathbb{P}}{d \mathbb{Q}} = e^{c W_T - c^2T/2}$. Then $(e^{-rt}S_t)_{0\le t\le T}$ is a $\mathbb{Q}$--martingale, i.e. $\mathbb{Q}$ is a risk--neutral measure for the Black--Scholes model.
\end{theorem}
\begin{proof}
    Let $\hat{W}_t = W_t - ct$ for $0\le t\le T$, so by Cameron--Martin, $(\hat{W}_t)_t$ is a $\mathbb{Q}$--Brownian motion. Now doing some algebra,
    \begin{align*}
        e^{-rt}S_t = S_0 e^{-\frac{\sigma^2}{2}t+\sigma \hat{W}_t},
    \end{align*}
    and this is a $\mathbb{Q}$--martingale by ES4 (compare this to the discrete--time proof on ES2).
\end{proof}

\begin{defn}
    Let $Y$ be the payout of a European contigent claim in the Black--Scholes model. The B--S price of the claim is \[
    \pi_t = e^{-r(T-t)}\mathbb{E}^\mathbb{Q}[Y \mid \mathcal{F}_t]
    \]
    for $0\le t\le T$.
\end{defn}
Note that $\pi_t e^{-rt} = e^{-rt} \mathbb{E}^\mathbb{Q}[Y \mid \mathcal{F}_t]$ is a martingale under $\mathbb{Q}$ as desired.
\begin{prop}
    Let $Y=g(S_T)$ be the payout of a vanilla claim. Then $\pi_t = V(t,S_t)$, where $Z\sim N(0,1)$ and \[
    V(t,s) = e^{-r(T-t)} \mathbb{E}\left[g(s e^{(r-\sigma^2/2)(T-t)+\sigma\sqrt{T-t}Z})\right].
    \]
\end{prop}
\begin{proof}
    \begin{align*}
        &\pi_t = e^{-r(T-t)} \mathbb{E}^\mathbb{Q}[g(S_T) \mid \mathcal{F}_t] \\
        =& e^{-r(T-t)} \mathbb{E}^\mathbb{Q}[g(S_t e^{(r-\sigma^2/2)(T-t) + \sigma(\hat{W}_T-\hat{W}_t)}) \mid \mathcal{F}_t].
    \end{align*}
    Now note $\hat{W}_T - \hat{W}_t$ is independent of $\mathcal{F}_t$ and has the same distribution as $\sqrt{T-t}Z$.
\end{proof}

By a (tedious) change of variables, it turns out that $V$ solves \[
\frac{\partial V}{\partial t} + s r \frac{\partial V}{\partial s} + \frac{1}{2}s ^2 \sigma^2 \frac{\partial^2 V}{\partial s ^2} = rV
\]
with boundary condition $V(T,s)=g(s) ~\forall s$.

This is known as the \textbf{Black--Scholes PDE}. 
\vspace{1mm}

\marginpar{30 Nov 2022, Lecture 24}

To recap: the stock price at time $t$ is $S_t=S_0 e^{\mu t + \sigma W_t}$ for $(W_t)_{t\ge 0}$ a Brownian motion. Then the Black--Scholes price of a vanilla claim is \[
\pi_t = e^{-r(T-t)}\mathbb{E}^\mathbb{Q}[g(S_t) \mid \mathcal{F}_t] = V(t,S_t),
\]
where \[
V(t,s) = e^{-r(T-t)}\mathbb{E}\left[g(s \exp \left((r-\frac{\sigma^2}{2})(T-t)+\sigma \sqrt{T-t} Z\right)\right]
\]
for $Z \sim N(0,1)$ a standard normal. This satisfies the Black--Scholes PDE \[
\frac{\partial V}{\partial t} + rs \frac{\partial V}{\partial s} + \frac{1}{2}s ^2 \sigma^2 \frac{\partial^2 V}{\partial s ^2} = rV.
\]
A way to derive this: notice that \[
V(t,s) = e^{-r \delta} \mathbb{E}[V(t+\delta,S_{t+\delta}) \mid S_t=s]
\]
(from the martingale property on $(e^{-rt}\pi_t)_{t\ge 0}$). Now \[
V(t,s) = e^{-r \delta} \mathbb{E}[V(t+\delta, s \xi)]
\]
for $\xi = \exp \left((r-\frac{\sigma^2}{2})\delta + \sigma \sqrt{\delta} Z\right)$. We Taylor expand this to get
\begin{align*}
    e^{r \delta}V(t,s) = \mathbb{E}[V(t,s)] + \frac{\partial V}{\partial t}\delta + \frac{\partial V}{\partial s}(s\xi -s) + \frac{1}{2}\frac{\partial^2 V}{\partial t^2}\delta^2 + \frac{\partial^2 V}{\partial t \partial s} \delta s (\xi-1) + \frac{1}{2}\frac{\partial^2 V}{\partial s ^2}s^2(\xi-1)^2 + \text{other terms}. 
\end{align*}
We get 
\begin{align*}
    &\mathbb{E}[\xi -1] = e^{r \delta} - 1 \approx r \delta \\
    &\mathbb{E}[(\xi-1)^2] = e^{(2r+\sigma^2)\delta}-2e^{r \delta} + 1 \approx \sigma ^2 \delta
\end{align*}
and the other terms are of lower degree order. Now divide by $\delta$, take the limit, and we get the above expression.
\vspace{1mm}

In the binomial model, to replicate the claim at time $t$ with respect to the event $\{S_{t-\delta}=s\}$, we have to hold \[
\frac{V(t,s(1+b))-V(t,s(1+a))}{s(b-a)} \approx \frac{\partial V}{\partial s}(t,s)
\]
shares of the underlying asset between times $t-\delta$ and $t$. These partial derivatives have names: $\frac{\partial V}{\partial s}$ is known as the \textbf{delta} of the claim and $\frac{\partial^2 V}{\partial s^2}$ is known as the \textbf{gamma}.\footnote{Gamma is the change in delta, so we can think of getting the right delta at some point in time and then changing it using gamma.} 
\begin{prop}
    If the payout function $g$ is increasing, then delta is nonnegative. If $g$ is convex, then gamma is nonnegative. 
\end{prop}
\begin{proof}
    By example sheet 1 and the formula $V(s,t) = e^{-r(T-t)}\mathbb{E}^\mathbb{Q}[\text{stuff}]$.
\end{proof}
\vspace{2mm}

The Black-Scholes price of a European call with maturity $T$ and strike $K$ is \[
\pi_t = S_t \Phi(d_1) - Ke^{-r(T-t)}\Phi(d_2)
\]
for 
\begin{align*}
    &d_1 = \frac{\log\frac{S_t}{K}}{\sigma\sqrt{T-t}}+(\frac{r}{\sigma}+\frac{\sigma}{2})\sqrt{T-t}\\
    &d_2 = \frac{\log\frac{S_t}{K}}{\sigma\sqrt{T-t}}+(\frac{r}{\sigma}-\frac{\sigma}{2})\sqrt{T-t}.
\end{align*}
Here $\Phi(x)=\int_{-\infty}^{\infty} \frac{e^{-s^2/2}}{\sqrt{2\pi}} \mathrm{d}s$, the CDF of the normal distribution.
\vspace{1mm}

\textbf{Derivation:} Let $\delta = T -t$ and $\xi = e^{(r-\frac{\sigma^2}{2})(T-t)+\sigma \sqrt{T-t} Z}$. Then
\begin{align*}
    V(t,s) &= e^{-r \delta}\mathbb{E}\left[(s\xi-K)^{+}\right] \\
    &= e^{-r \delta}\mathbb{E}[(s \xi -K) \mathbbm{1}_{\xi > K/s}] \\
    &= s \mathbb{E}[e^{-r \delta}\xi \mathbbm{1}_{\xi > K/s}] - e^{-r \delta}K \mathbb{P}(\xi > K/s).
\end{align*}
Note $\mathbb{P}(\xi > K/s) = 1 - \Phi(-d_2) = \Phi(d_2)$, and by change of variables formula for normal random variables (as on ES1), the law of $\xi$ under $\overline{\mathbb{P}}$ is the same as the law of $e^{\sigma^2 \delta}\xi$ under $\mathbb{P}$ (here $\frac{\overline{\mathbb{P}}}{\mathbb{P}}=e^{-r \delta}\xi$), so \[
\mathbb{E}[e^{-r \delta}\mathbbm{1}_{\{\xi > K/s\}}] = \mathbb{P}(\xi > Ke^{-\sigma^2 \delta}/s) = 1 - \Phi(-d_2 - \sigma\sqrt{\delta}) = \Phi(d_1)
\]
and we're done.
\begin{prop}
    The delta of a European call option is $\frac{\partial V}{\partial s} = \Phi(d_1)$.
\end{prop}
\begin{proof}
    Since $\frac{\partial }{\partial x} (x-K)^{+} = \mathbbm{1}_{\{x > K\}}$, we get
    \[
    \frac{\partial V}{\partial s} = e^{-r(T-t)}\mathbb{E}[\mathbbm{1}_{\{Z>-d_2\}}e^{(r-\frac{\sigma^2}{2})(T-t)+\sigma\sqrt{T-t}Z}] = \Phi(d_1).
    \]
\end{proof}
\vspace{2mm}

\textbf{Barriers:} We can also price some exotic barrier--type options. Given the payout $Y$ of a European claim and a given barrier $B$, the:
\begin{itemize}
    \item down--and--out version of the claim is a European claim $Y \mathbbm{1}_{\inf_{0\le t\le T}S_t >B}$ (i.e. if the price dips below some level, you don't get a payout (an opportunity to buy/sell)).
    \item up--and--out version is $Y \mathbbm{1}_{\max_{0\le t\le T}S_t <B}$.
    \item up--and--in version is $Y \mathbbm{1}_{\max_{0\le t\le T}S_t >B}$.
    \item down--and--in version is $Y \mathbbm{1}_{\inf_{0\le t\le T}S_t \le B}$.
\end{itemize} 
\begin{prop}
    The Black--Scholes price of an up--and--out with payout $g(S_T)\mathbbm{1}_{\{\max S_t < B\}}$ is the same as that of a vanilla claim with payout $$g(S_T)\mathbbm{1}_{\{S_T < b\}} - \left(\frac{B}{S_0}\right)^{\frac{2r}{\sigma^2}-1}g\left(\frac{B^2S_T}{S_0^2}\right)\mathbbm{1}_{\{S_T\le \frac{S_0^2}{B}\}}.$$
\end{prop}
\begin{proof}
    See official lecture notes.
\end{proof}
\end{document}

